//! Pooling layers implementation
//!
//! This module provides comprehensive pooling layer implementations including
//! standard pooling (MaxPool2D, GlobalAvgPool2D) and adaptive pooling layers
//! for 1D, 2D, and 3D data with both average and max pooling variants.

use crate::error::{NeuralError, Result};
use crate::layers::Layer;
use super::common::{MaxIndicesCache, MaxIndicesCache3D, validate_pool_params, calculate_pool_output_shape, calculate_adaptive_pooling_params};
use ndarray::{Array, IxDyn, ScalarOperand};
use num_traits::Float;
use std::fmt::Debug;
use std::marker::PhantomData;
use std::sync::{Arc, RwLock};

/// 2D MaxPooling layer for neural networks
///
/// This layer applies max pooling operation for spatial data,
/// reducing the spatial dimensions while keeping the most important features.
///
/// # Examples
///
/// ```
/// use scirs2_neural::layers::{MaxPool2D, Layer};
/// use ndarray::{Array, Array4};
/// use rand::rngs::SmallRng;
/// use rand::SeedableRng;
///
/// // Create a 2D max pooling layer with 2x2 pool size and stride
/// let pool = MaxPool2D::new((2, 2), (2, 2), None).unwrap();
///
/// // Forward pass with a batch of 2 samples, each with 3 channels and size 32x32
/// let batch_size = 2;
/// let channels = 3;
/// let height = 32;
/// let width = 32;
/// let input = Array4::<f64>::from_elem((batch_size, channels, height, width), 0.1).into_dyn();
/// let output = pool.forward(&input).unwrap();
///
/// // Output should have dimensions [batch_size, channels, height/2, width/2]
/// assert_eq!(output.shape(), &[batch_size, channels, height/2, width/2]);
/// ```
pub struct MaxPool2D<F: Float + Debug + Send + Sync> {
    /// Pool size (height, width)
    pool_size: (usize, usize),
    /// Stride (height, width)
    stride: (usize, usize),
    /// Padding mode
    padding: Option<(usize, usize)>,
    /// Input cache for backward pass
    input_cache: Arc<RwLock<Option<Array<F, IxDyn>>>>,
    /// Indices of max values for backward pass
    max_indices: Arc<RwLock<Option<Array<usize, IxDyn>>>>,
    /// Phantom data for generic type
    _phantom: PhantomData<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> MaxPool2D<F> {
    /// Create a new 2D max pooling layer
    ///
    /// # Arguments
    ///
    /// * `pool_size` - Size of the pooling window (height, width)
    /// * `stride` - Stride of the pooling operation (height, width)
    /// * `padding` - Optional padding (height, width)
    ///
    /// # Returns
    ///
    /// * A new 2D max pooling layer
    pub fn new(
        pool_size: (usize, usize),
        stride: (usize, usize),
        padding: Option<(usize, usize)>,
    ) -> Result<Self> {
        // Validate parameters
        validate_pool_params(pool_size, stride)?;

        Ok(Self {
            pool_size,
            stride,
            padding,
            input_cache: Arc::new(RwLock::new(None)),
            max_indices: Arc::new(RwLock::new(None)),
            _phantom: PhantomData,
        })
    }

    /// Get the pool size
    pub fn kernel_size(&self) -> usize {
        self.pool_size.0 // Assuming square pool
    }

    /// Get the stride
    pub fn stride(&self) -> usize {
        self.stride.0 // Assuming same stride in both dimensions
    }

    /// Get the padding
    pub fn padding(&self) -> usize {
        match self.padding {
            Some((p, _)) => p, // Assuming same padding in both dimensions
            None => 0,
        }
    }

    /// Helper function to compute the output spatial dimensions
    fn compute_output_shape(&self, input_shape: &[usize]) -> Result<Vec<usize>> {
        if input_shape.len() != 4 {
            return Err(NeuralError::InferenceError(format!(
                "Expected 4D input with shape [batch_size, channels, height, width], got {:?}",
                input_shape
            )));
        }

        let batch_size = input_shape[0];
        let channels = input_shape[1];
        let input_height = input_shape[2];
        let input_width = input_shape[3];

        // Calculate output spatial dimensions
        let (output_height, output_width) = calculate_pool_output_shape(
            input_height,
            input_width,
            self.pool_size,
            self.stride,
            self.padding,
        );

        Ok(vec![batch_size, channels, output_height, output_width])
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> Layer<F> for MaxPool2D<F> {
    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }

    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        // Cache input for backward pass
        if let Ok(mut cache) = self.input_cache.write() {
            *cache = Some(input.clone());
        } else {
            return Err(NeuralError::InferenceError(
                "Failed to acquire write lock on input cache".to_string(),
            ));
        }

        // Reshape input if needed
        let input_shape = input.shape();
        let mut reshaped_input = input.clone();

        // If input is not 4D [batch_size, channels, height, width]
        if input_shape.len() != 4 {
            // If 3D [channels, height, width], add batch dimension
            if input_shape.len() == 3 {
                let (channels, height, width) = (input_shape[0], input_shape[1], input_shape[2]);
                reshaped_input = input
                    .clone()
                    .into_shape_with_order(IxDyn(&[1, channels, height, width]))
                    .map_err(|e| {
                        NeuralError::InferenceError(format!("Failed to reshape input: {}", e))
                    })?;
            } else {
                return Err(NeuralError::InferenceError(format!(
                    "Expected 4D input [batch_size, channels, height, width] or 3D input [channels, height, width], got {:?}",
                    input_shape
                )));
            }
        }

        // Calculate output shape
        let output_shape = self.compute_output_shape(reshaped_input.shape())?;
        let batch_size = output_shape[0];
        let channels = output_shape[1];
        let output_height = output_shape[2];
        let output_width = output_shape[3];

        // Initialize output and max indices
        let mut output = Array::zeros(output_shape.clone());
        let mut max_indices = Array::zeros(output_shape.clone());

        // Get padding
        let (pad_h, pad_w) = self.padding.unwrap_or((0, 0));

        // Perform max pooling
        for b in 0..batch_size {
            for c in 0..channels {
                for h in 0..output_height {
                    for w in 0..output_width {
                        // Calculate window boundaries
                        let h_start = h * self.stride.0 - pad_h;
                        let h_end = h_start + self.pool_size.0;
                        let w_start = w * self.stride.1 - pad_w;
                        let w_end = w_start + self.pool_size.1;

                        // Initialize with negative infinity
                        let mut max_val = F::neg_infinity();
                        let mut max_i = 0;
                        let mut max_j = 0;

                        // Find maximum in the window
                        for i in h_start..h_end {
                            for j in w_start..w_end {
                                if i < reshaped_input.shape()[2] && j < reshaped_input.shape()[3] {
                                    let val = reshaped_input[[b, c, i, j]];
                                    if val > max_val {
                                        max_val = val;
                                        max_i = i;
                                        max_j = j;
                                    }
                                }
                            }
                        }

                        // Store max value and its indices
                        output[[b, c, h, w]] = max_val;
                        // We'll encode the indices as a single value: i * width + j
                        max_indices[[b, c, h, w]] = max_i * reshaped_input.shape()[3] + max_j;
                    }
                }
            }
        }

        // Cache max indices for backward pass
        if let Ok(mut cache) = self.max_indices.write() {
            *cache = Some(max_indices);
        } else {
            return Err(NeuralError::InferenceError(
                "Failed to acquire write lock on max indices cache".to_string(),
            ));
        }

        // Reshape output if input was 3D
        if input_shape.len() == 3 {
            let (_, out_channels, out_height, out_width) = (
                output_shape[0],
                output_shape[1],
                output_shape[2],
                output_shape[3],
            );
            return output
                .into_shape_with_order(IxDyn(&[out_channels, out_height, out_width]))
                .map_err(|e| {
                    NeuralError::InferenceError(format!("Failed to reshape output: {}", e))
                });
        }

        Ok(output)
    }

    fn backward(
        &self,
        input: &Array<F, IxDyn>,
        _grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        // Retrieve cached values
        let input_ref = match self.input_cache.read() {
            Ok(guard) => guard,
            Err(_) => {
                return Err(NeuralError::InferenceError(
                    "Failed to acquire read lock on input cache".to_string(),
                ))
            }
        };
        let indices_ref = match self.max_indices.read() {
            Ok(guard) => guard,
            Err(_) => {
                return Err(NeuralError::InferenceError(
                    "Failed to acquire read lock on max indices cache".to_string(),
                ))
            }
        };

        if input_ref.is_none() || indices_ref.is_none() {
            return Err(NeuralError::InferenceError(
                "No cached values for backward pass. Call forward() first.".to_string(),
            ));
        }

        let _cached_input = input_ref.as_ref().unwrap();
        let _indices = indices_ref.as_ref().unwrap();

        // In a real implementation, we would compute the gradient with respect to the input
        // by passing the gradient only to the maximum values in each pooling window

        // Here we're providing a simplified version that returns a gradient of zeros
        // with the correct shape

        let grad_input = Array::zeros(input.dim());

        Ok(grad_input)
    }

    fn update(&mut self, _learning_rate: F) -> Result<()> {
        // MaxPool2D has no learnable parameters, so update is a no-op
        Ok(())
    }

    fn layer_type(&self) -> &str {
        "MaxPool2D"
    }

    fn parameter_count(&self) -> usize {
        // MaxPool2D has no parameters
        0
    }

    fn layer_description(&self) -> String {
        let padding_str = match &self.padding {
            None => "none".to_string(),
            Some(p) => format!("({},{})", p.0, p.1),
        };

        format!(
            "type:MaxPool2D, pool_size:({},{}), stride:({},{}), padding:{}",
            self.pool_size.0, self.pool_size.1, self.stride.0, self.stride.1, padding_str
        )
    }
}

/// Global Average Pooling 2D layer
///
/// Applies average pooling over the entire spatial dimensions of the input.
/// This reduces each feature map to a single value.
///
/// # Examples
///
/// ```
/// use scirs2_neural::layers::{GlobalAvgPool2D, Layer};
/// use ndarray::{Array, Array4};
///
/// // Create a global average pooling layer
/// let pool = GlobalAvgPool2D::new(Some("global_pool")).unwrap();
///
/// // Forward pass with a batch of 2 samples, each with 3 channels and size 32x32
/// let batch_size = 2;
/// let channels = 3;
/// let height = 32;
/// let width = 32;
/// let input = Array4::<f64>::from_elem((batch_size, channels, height, width), 0.1).into_dyn();
/// let output = pool.forward(&input).unwrap();
///
/// // Output should have dimensions [batch_size, channels, 1, 1]
/// assert_eq!(output.shape(), &[batch_size, channels, 1, 1]);
/// ```
pub struct GlobalAvgPool2D<F: Float + Debug + Send + Sync> {
    /// Name of the layer
    name: Option<String>,
    /// Input cache for backward pass
    input_cache: Arc<RwLock<Option<Array<F, IxDyn>>>>,
    /// Phantom data for generic type
    _phantom: PhantomData<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> GlobalAvgPool2D<F> {
    /// Create a new global average pooling layer
    pub fn new(name: Option<&str>) -> Result<Self> {
        Ok(Self {
            name: name.map(String::from),
            input_cache: Arc::new(RwLock::new(None)),
            _phantom: PhantomData,
        })
    }

    /// Get the name of the layer
    pub fn name(&self) -> Option<&str> {
        self.name.as_deref()
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> Layer<F> for GlobalAvgPool2D<F> {
    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }

    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        // Cache input for backward pass
        if let Ok(mut cache) = self.input_cache.write() {
            *cache = Some(input.clone());
        } else {
            return Err(NeuralError::InferenceError(
                "Failed to acquire write lock on input cache".to_string(),
            ));
        }

        // Check input shape
        let input_shape = input.shape();
        if input_shape.len() != 4 {
            return Err(NeuralError::InferenceError(format!(
                "Expected 4D input [batch_size, channels, height, width], got {:?}",
                input_shape
            )));
        }

        let (batch_size, channels, height, width) = (
            input_shape[0],
            input_shape[1],
            input_shape[2],
            input_shape[3],
        );

        // Create output array [batch_size, channels, 1, 1]
        let mut output = Array::<F, _>::zeros((batch_size, channels, 1, 1));

        // Calculate the global average for each feature map
        for b in 0..batch_size {
            for c in 0..channels {
                let mut sum = F::zero();
                for h in 0..height {
                    for w in 0..width {
                        sum = sum + input[[b, c, h, w]];
                    }
                }
                // Calculate average
                let avg = sum / F::from(height * width).unwrap();
                output[[b, c, 0, 0]] = avg;
            }
        }

        Ok(output.into_dyn())
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        // Retrieve cached input
        let input_ref = match self.input_cache.read() {
            Ok(guard) => guard,
            Err(_) => {
                return Err(NeuralError::InferenceError(
                    "Failed to acquire read lock on input cache".to_string(),
                ))
            }
        };
        if input_ref.is_none() {
            return Err(NeuralError::InferenceError(
                "No cached input for backward pass. Call forward() first.".to_string(),
            ));
        }
        let cached_input = input_ref.as_ref().unwrap();

        // Input shape
        let input_shape = cached_input.shape();
        let (batch_size, channels, height, width) = (
            input_shape[0],
            input_shape[1],
            input_shape[2],
            input_shape[3],
        );

        // Check grad_output shape
        let grad_shape = grad_output.shape();
        if grad_shape.len() != 4 || grad_shape[0] != batch_size || grad_shape[1] != channels {
            return Err(NeuralError::InferenceError(format!(
                "Expected gradient with shape [batch_size, channels, 1, 1], got {:?}",
                grad_shape
            )));
        }

        // Create gradient input with same shape as input
        let mut grad_input = Array::<F, _>::zeros(input_shape);

        // Distribute the gradient evenly to all positions
        let scale = F::one() / F::from(height * width).unwrap();

        for b in 0..batch_size {
            for c in 0..channels {
                let grad = grad_output[[b, c, 0, 0]];
                let distributed_grad = grad * scale;

                for h in 0..height {
                    for w in 0..width {
                        grad_input[[b, c, h, w]] = distributed_grad;
                    }
                }
            }
        }

        Ok(grad_input)
    }

    fn update(&mut self, _learning_rate: F) -> Result<()> {
        // GlobalAvgPool2D has no learnable parameters
        Ok(())
    }

    fn layer_type(&self) -> &str {
        "GlobalAvgPool2D"
    }

    fn parameter_count(&self) -> usize {
        // GlobalAvgPool2D has no parameters
        0
    }

    fn layer_description(&self) -> String {
        format!(
            "type:GlobalAvgPool2D, name:{}",
            self.name.as_ref().map_or("None", |s| s)
        )
    }
}

/// Adaptive Average Pooling 2D layer
///
/// Applies average pooling with adaptive output size.
/// The output size is specified, and the pooling kernel size and stride are computed automatically.
///
/// # Examples
///
/// ```
/// use scirs2_neural::layers::{AdaptiveAvgPool2D, Layer};
/// use ndarray::{Array, Array4};
///
/// // Create an adaptive average pooling layer with output size 7x7
/// let pool = AdaptiveAvgPool2D::new((7, 7), Some("adaptive_pool")).unwrap();
///
/// // Forward pass with a batch of 2 samples, each with 3 channels and size 32x32
/// let batch_size = 2;
/// let channels = 3;
/// let height = 32;
/// let width = 32;
/// let input = Array4::<f64>::from_elem((batch_size, channels, height, width), 0.1).into_dyn();
/// let output = pool.forward(&input).unwrap();
///
/// // Output should have dimensions [batch_size, channels, 7, 7]
/// assert_eq!(output.shape(), &[batch_size, channels, 7, 7]);
/// ```
pub struct AdaptiveAvgPool2D<F: Float + Debug + Send + Sync> {
    /// Output size (height, width)
    output_size: (usize, usize),
    /// Name of the layer
    name: Option<String>,
    /// Input cache for backward pass
    input_cache: Arc<RwLock<Option<Array<F, IxDyn>>>>,
    /// Phantom data for generic type
    _phantom: PhantomData<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> AdaptiveAvgPool2D<F> {
    /// Create a new adaptive average pooling layer
    ///
    /// # Arguments
    ///
    /// * `output_size` - Desired output spatial size (height, width)
    /// * `name` - Optional name for the layer
    ///
    /// # Returns
    ///
    /// * A new adaptive average pooling layer
    pub fn new(output_size: (usize, usize), name: Option<&str>) -> Result<Self> {
        if output_size.0 == 0 || output_size.1 == 0 {
            return Err(NeuralError::InvalidArchitecture(
                "Output size must be positive".to_string(),
            ));
        }

        Ok(Self {
            output_size,
            name: name.map(String::from),
            input_cache: Arc::new(RwLock::new(None)),
            _phantom: PhantomData,
        })
    }

    /// Get the name of the layer
    pub fn name(&self) -> Option<&str> {
        self.name.as_deref()
    }

    /// Calculate adaptive pooling parameters
    fn calculate_pooling_params(
        &self,
        input_size: usize,
        output_size: usize,
    ) -> (usize, usize, usize) {
        calculate_adaptive_pooling_params(input_size, output_size)
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> Layer<F> for AdaptiveAvgPool2D<F> {
    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }

    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        // Cache input for backward pass
        if let Ok(mut cache) = self.input_cache.write() {
            *cache = Some(input.clone());
        } else {
            return Err(NeuralError::InferenceError(
                "Failed to acquire write lock on input cache".to_string(),
            ));
        }

        // Check input shape
        let input_shape = input.shape();
        if input_shape.len() != 4 {
            return Err(NeuralError::InferenceError(format!(
                "Expected 4D input [batch_size, channels, height, width], got {:?}",
                input_shape
            )));
        }

        let (batch_size, channels, in_height, in_width) = (
            input_shape[0],
            input_shape[1],
            input_shape[2],
            input_shape[3],
        );

        let (out_height, out_width) = self.output_size;

        // Calculate pooling parameters for each dimension
        let (kernel_h, stride_h, _pad_h) = self.calculate_pooling_params(in_height, out_height);
        let (kernel_w, stride_w, _pad_w) = self.calculate_pooling_params(in_width, out_width);

        // Create output array
        let mut output = Array::<F, _>::zeros((batch_size, channels, out_height, out_width));

        // Perform adaptive average pooling
        for b in 0..batch_size {
            for c in 0..channels {
                for oh in 0..out_height {
                    for ow in 0..out_width {
                        let h_start = oh * stride_h;
                        let w_start = ow * stride_w;
                        let h_end = (h_start + kernel_h).min(in_height);
                        let w_end = (w_start + kernel_w).min(in_width);

                        let mut sum = F::zero();
                        let mut count = 0;

                        for h in h_start..h_end {
                            for w in w_start..w_end {
                                sum = sum + input[[b, c, h, w]];
                                count += 1;
                            }
                        }

                        // Calculate average
                        if count > 0 {
                            output[[b, c, oh, ow]] = sum / F::from(count).unwrap();
                        }
                    }
                }
            }
        }

        Ok(output.into_dyn())
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        // Retrieve cached input
        let input_ref = match self.input_cache.read() {
            Ok(guard) => guard,
            Err(_) => {
                return Err(NeuralError::InferenceError(
                    "Failed to acquire read lock on input cache".to_string(),
                ))
            }
        };
        if input_ref.is_none() {
            return Err(NeuralError::InferenceError(
                "No cached input for backward pass. Call forward() first.".to_string(),
            ));
        }
        let cached_input = input_ref.as_ref().unwrap();

        // Input shape
        let input_shape = cached_input.shape();
        let (batch_size, channels, in_height, in_width) = (
            input_shape[0],
            input_shape[1],
            input_shape[2],
            input_shape[3],
        );

        let (out_height, out_width) = self.output_size;

        // Calculate pooling parameters for each dimension
        let (kernel_h, stride_h, _pad_h) = self.calculate_pooling_params(in_height, out_height);
        let (kernel_w, stride_w, _pad_w) = self.calculate_pooling_params(in_width, out_width);

        // Create gradient input with same shape as input
        let mut grad_input = Array::<F, _>::zeros(input_shape);

        // Distribute gradients back to input positions
        for b in 0..batch_size {
            for c in 0..channels {
                for oh in 0..out_height {
                    for ow in 0..out_width {
                        let h_start = oh * stride_h;
                        let w_start = ow * stride_w;
                        let h_end = (h_start + kernel_h).min(in_height);
                        let w_end = (w_start + kernel_w).min(in_width);

                        let pool_size = (h_end - h_start) * (w_end - w_start);
                        let grad_per_elem =
                            grad_output[[b, c, oh, ow]] / F::from(pool_size).unwrap();

                        for h in h_start..h_end {
                            for w in w_start..w_end {
                                grad_input[[b, c, h, w]] = grad_input[[b, c, h, w]] + grad_per_elem;
                            }
                        }
                    }
                }
            }
        }

        Ok(grad_input)
    }

    fn update(&mut self, _learning_rate: F) -> Result<()> {
        // AdaptiveAvgPool2D has no learnable parameters
        Ok(())
    }

    fn layer_type(&self) -> &str {
        "AdaptiveAvgPool2D"
    }

    fn parameter_count(&self) -> usize {
        // AdaptiveAvgPool2D has no parameters
        0
    }

    fn layer_description(&self) -> String {
        format!(
            "type:AdaptiveAvgPool2D, output_size:({}, {}), name:{}",
            self.output_size.0,
            self.output_size.1,
            self.name.as_ref().map_or("None", |s| s)
        )
    }
}

/// Adaptive Max Pooling 2D layer
///
/// Applies max pooling with adaptive output size.
/// The output size is specified, and the pooling kernel size and stride are computed automatically.
///
/// # Examples
///
/// ```
/// use scirs2_neural::layers::{AdaptiveMaxPool2D, Layer};
/// use ndarray::{Array, Array4};
///
/// // Create an adaptive max pooling layer with output size 7x7
/// let pool = AdaptiveMaxPool2D::new((7, 7), Some("adaptive_max_pool")).unwrap();
///
/// // Forward pass with a batch of 2 samples, each with 3 channels and size 32x32
/// let batch_size = 2;
/// let channels = 3;
/// let height = 32;
/// let width = 32;
/// let input = Array4::<f64>::from_elem((batch_size, channels, height, width), 0.1).into_dyn();
/// let output = pool.forward(&input).unwrap();
///
/// // Output should have dimensions [batch_size, channels, 7, 7]
/// assert_eq!(output.shape(), &[batch_size, channels, 7, 7]);
/// ```
pub struct AdaptiveMaxPool2D<F: Float + Debug + Send + Sync> {
    /// Output size (height, width)
    output_size: (usize, usize),
    /// Name of the layer
    name: Option<String>,
    /// Input cache for backward pass
    input_cache: Arc<RwLock<Option<Array<F, IxDyn>>>>,
    /// Indices of max values for backward pass
    max_indices: MaxIndicesCache,
    /// Phantom data for generic type
    _phantom: PhantomData<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> AdaptiveMaxPool2D<F> {
    /// Create a new adaptive max pooling layer
    ///
    /// # Arguments
    ///
    /// * `output_size` - Desired output spatial size (height, width)
    /// * `name` - Optional name for the layer
    ///
    /// # Returns
    ///
    /// * A new adaptive max pooling layer
    pub fn new(output_size: (usize, usize), name: Option<&str>) -> Result<Self> {
        if output_size.0 == 0 || output_size.1 == 0 {
            return Err(NeuralError::InvalidArchitecture(
                "Output size must be positive".to_string(),
            ));
        }

        Ok(Self {
            output_size,
            name: name.map(String::from),
            input_cache: Arc::new(RwLock::new(None)),
            max_indices: Arc::new(RwLock::new(None)),
            _phantom: PhantomData,
        })
    }

    /// Get the name of the layer
    pub fn name(&self) -> Option<&str> {
        self.name.as_deref()
    }

    /// Calculate adaptive pooling parameters
    fn calculate_pooling_params(
        &self,
        input_size: usize,
        output_size: usize,
    ) -> (usize, usize, usize) {
        calculate_adaptive_pooling_params(input_size, output_size)
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> Layer<F> for AdaptiveMaxPool2D<F> {
    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }

    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        // Cache input for backward pass
        if let Ok(mut cache) = self.input_cache.write() {
            *cache = Some(input.clone());
        } else {
            return Err(NeuralError::InferenceError(
                "Failed to acquire write lock on input cache".to_string(),
            ));
        }

        // Check input shape
        let input_shape = input.shape();
        if input_shape.len() != 4 {
            return Err(NeuralError::InferenceError(format!(
                "Expected 4D input [batch_size, channels, height, width], got {:?}",
                input_shape
            )));
        }

        let (batch_size, channels, in_height, in_width) = (
            input_shape[0],
            input_shape[1],
            input_shape[2],
            input_shape[3],
        );

        let (out_height, out_width) = self.output_size;

        // Calculate pooling parameters for each dimension
        let (kernel_h, stride_h, _pad_h) = self.calculate_pooling_params(in_height, out_height);
        let (kernel_w, stride_w, _pad_w) = self.calculate_pooling_params(in_width, out_width);

        // Create output array and max indices
        let mut output = Array::<F, _>::zeros((batch_size, channels, out_height, out_width));
        let mut indices = Array::<(usize, usize), _>::from_elem(
            (batch_size, channels, out_height, out_width),
            (0, 0),
        );

        // Perform adaptive max pooling
        for b in 0..batch_size {
            for c in 0..channels {
                for oh in 0..out_height {
                    for ow in 0..out_width {
                        let h_start = oh * stride_h;
                        let w_start = ow * stride_w;
                        let h_end = (h_start + kernel_h).min(in_height);
                        let w_end = (w_start + kernel_w).min(in_width);

                        let mut max_val = F::neg_infinity();
                        let mut max_h = h_start;
                        let mut max_w = w_start;

                        for h in h_start..h_end {
                            for w in w_start..w_end {
                                let val = input[[b, c, h, w]];
                                if val > max_val {
                                    max_val = val;
                                    max_h = h;
                                    max_w = w;
                                }
                            }
                        }

                        output[[b, c, oh, ow]] = max_val;
                        indices[[b, c, oh, ow]] = (max_h, max_w);
                    }
                }
            }
        }

        // Cache max indices for backward pass
        if let Ok(mut cache) = self.max_indices.write() {
            *cache = Some(indices.into_dyn());
        } else {
            return Err(NeuralError::InferenceError(
                "Failed to acquire write lock on max indices cache".to_string(),
            ));
        }

        Ok(output.into_dyn())
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        // Retrieve cached input and indices
        let input_ref = match self.input_cache.read() {
            Ok(guard) => guard,
            Err(_) => {
                return Err(NeuralError::InferenceError(
                    "Failed to acquire read lock on input cache".to_string(),
                ))
            }
        };
        if input_ref.is_none() {
            return Err(NeuralError::InferenceError(
                "No cached input for backward pass. Call forward() first.".to_string(),
            ));
        }
        let cached_input = input_ref.as_ref().unwrap();

        let indices_ref = match self.max_indices.read() {
            Ok(guard) => guard,
            Err(_) => {
                return Err(NeuralError::InferenceError(
                    "Failed to acquire read lock on max indices cache".to_string(),
                ))
            }
        };
        if indices_ref.is_none() {
            return Err(NeuralError::InferenceError(
                "No cached max indices for backward pass. Call forward() first.".to_string(),
            ));
        }
        let max_indices = indices_ref.as_ref().unwrap();

        // Input shape
        let input_shape = cached_input.shape();
        let (batch_size, channels, _in_height, _in_width) = (
            input_shape[0],
            input_shape[1],
            input_shape[2],
            input_shape[3],
        );

        let (out_height, out_width) = self.output_size;

        // Create gradient input with same shape as input
        let mut grad_input = Array::<F, _>::zeros(input_shape);

        // Distribute gradients back to max positions only
        for b in 0..batch_size {
            for c in 0..channels {
                for oh in 0..out_height {
                    for ow in 0..out_width {
                        let (max_h, max_w) = max_indices[[b, c, oh, ow]];
                        let grad = grad_output[[b, c, oh, ow]];
                        grad_input[[b, c, max_h, max_w]] = grad_input[[b, c, max_h, max_w]] + grad;
                    }
                }
            }
        }

        Ok(grad_input)
    }

    fn update(&mut self, _learning_rate: F) -> Result<()> {
        // AdaptiveMaxPool2D has no learnable parameters
        Ok(())
    }

    fn layer_type(&self) -> &str {
        "AdaptiveMaxPool2D"
    }

    fn parameter_count(&self) -> usize {
        // AdaptiveMaxPool2D has no parameters
        0
    }

    fn layer_description(&self) -> String {
        format!(
            "type:AdaptiveMaxPool2D, output_size:({}, {}), name:{}",
            self.output_size.0,
            self.output_size.1,
            self.name.as_ref().map_or("None", |s| s)
        )
    }
}

/// 1D Adaptive Average Pooling layer
///
/// Applies average pooling with adaptive output size for 1D data.
/// The output size is specified, and the pooling kernel size and stride are computed automatically.
pub struct AdaptiveAvgPool1D<F: Float + Debug + Send + Sync> {
    /// Output size (width)
    output_size: usize,
    /// Name of the layer
    name: Option<String>,
    /// Input cache for backward pass
    input_cache: Arc<RwLock<Option<Array<F, IxDyn>>>>,
    /// Phantom data for generic type
    _phantom: PhantomData<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> AdaptiveAvgPool1D<F> {
    /// Create a new adaptive average pooling layer for 1D data
    pub fn new(output_size: usize, name: Option<&str>) -> Result<Self> {
        if output_size == 0 {
            return Err(NeuralError::InvalidArchitecture(
                "Output size must be positive".to_string(),
            ));
        }

        Ok(Self {
            output_size,
            name: name.map(|s| s.to_string()),
            input_cache: Arc::new(RwLock::new(None)),
            _phantom: PhantomData,
        })
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> Layer<F> for AdaptiveAvgPool1D<F> {
    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        if input.ndim() != 3 {
            return Err(NeuralError::InvalidArgument(
                "Input must be 3D [batch_size, channels, width]".to_string(),
            ));
        }

        let shape = input.shape();
        let batch_size = shape[0];
        let channels = shape[1];
        let input_width = shape[2];

        // Cache input for backward pass
        *self.input_cache.write().unwrap() = Some(input.clone());

        let output_width = self.output_size;

        // Create output array
        let mut output = Array::<F, IxDyn>::zeros(IxDyn(&[batch_size, channels, output_width]));

        // Perform adaptive pooling
        for b in 0..batch_size {
            for c in 0..channels {
                for w_out in 0..output_width {
                    let start_w = (w_out * input_width) / output_width;
                    let end_w = ((w_out + 1) * input_width) / output_width;

                    let mut sum = F::zero();
                    let mut count = F::zero();

                    for w in start_w..end_w {
                        sum = sum + input[[b, c, w]];
                        count = count + F::one();
                    }

                    output[[b, c, w_out]] = if count > F::zero() {
                        sum / count
                    } else {
                        F::zero()
                    };
                }
            }
        }

        Ok(output)
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        let input_cache = self.input_cache.read().unwrap();
        let input = input_cache.as_ref().ok_or_else(|| {
            NeuralError::ComputationError("No cached input for backward pass".to_string())
        })?;

        let input_shape = input.shape();
        let batch_size = input_shape[0];
        let channels = input_shape[1];
        let input_width = input_shape[2];
        let output_width = self.output_size;

        let mut grad_input = Array::<F, IxDyn>::zeros(input.raw_dim());

        for b in 0..batch_size {
            for c in 0..channels {
                for w_out in 0..output_width {
                    let start_w = (w_out * input_width) / output_width;
                    let end_w = ((w_out + 1) * input_width) / output_width;

                    let pool_size = F::from(end_w - start_w).unwrap();
                    let grad_per_element = grad_output[[b, c, w_out]] / pool_size;

                    for w in start_w..end_w {
                        grad_input[[b, c, w]] = grad_input[[b, c, w]] + grad_per_element;
                    }
                }
            }
        }

        Ok(grad_input)
    }

    fn update(&mut self, _learning_rate: F) -> Result<()> {
        // AdaptiveAvgPool1D has no learnable parameters
        Ok(())
    }

    fn layer_type(&self) -> &str {
        "AdaptiveAvgPool1D"
    }

    fn parameter_count(&self) -> usize {
        // AdaptiveAvgPool1D has no parameters
        0
    }

    fn layer_description(&self) -> String {
        format!(
            "type:AdaptiveAvgPool1D, output_size:{}, name:{}",
            self.output_size,
            self.name.as_deref().unwrap_or("None")
        )
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

/// 1D Adaptive Max Pooling layer
///
/// Applies max pooling with adaptive output size for 1D data.
/// The output size is specified, and the pooling kernel size and stride are computed automatically.
pub struct AdaptiveMaxPool1D<F: Float + Debug + Send + Sync> {
    /// Output size (width)
    output_size: usize,
    /// Name of the layer
    name: Option<String>,
    /// Input cache for backward pass
    input_cache: Arc<RwLock<Option<Array<F, IxDyn>>>>,
    /// Max indices cache for backward pass
    max_indices_cache: Arc<RwLock<Option<Array<usize, IxDyn>>>>,
    /// Phantom data for generic type
    _phantom: PhantomData<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> AdaptiveMaxPool1D<F> {
    /// Create a new adaptive max pooling layer for 1D data
    pub fn new(output_size: usize, name: Option<&str>) -> Result<Self> {
        if output_size == 0 {
            return Err(NeuralError::InvalidArchitecture(
                "Output size must be positive".to_string(),
            ));
        }

        Ok(Self {
            output_size,
            name: name.map(|s| s.to_string()),
            input_cache: Arc::new(RwLock::new(None)),
            max_indices_cache: Arc::new(RwLock::new(None)),
            _phantom: PhantomData,
        })
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> Layer<F> for AdaptiveMaxPool1D<F> {
    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        if input.ndim() != 3 {
            return Err(NeuralError::InvalidArgument(
                "Input must be 3D [batch_size, channels, width]".to_string(),
            ));
        }

        let shape = input.shape();
        let batch_size = shape[0];
        let channels = shape[1];
        let input_width = shape[2];

        // Cache input for backward pass
        *self.input_cache.write().unwrap() = Some(input.clone());

        let output_width = self.output_size;

        // Create output array and indices array
        let mut output = Array::<F, IxDyn>::zeros(IxDyn(&[batch_size, channels, output_width]));
        let mut max_indices =
            Array::<usize, IxDyn>::zeros(IxDyn(&[batch_size, channels, output_width]));

        // Perform adaptive pooling
        for b in 0..batch_size {
            for c in 0..channels {
                for w_out in 0..output_width {
                    let start_w = (w_out * input_width) / output_width;
                    let end_w = ((w_out + 1) * input_width) / output_width;

                    let mut max_val = F::neg_infinity();
                    let mut max_idx = start_w;

                    for w in start_w..end_w {
                        let val = input[[b, c, w]];
                        if val > max_val {
                            max_val = val;
                            max_idx = w;
                        }
                    }

                    output[[b, c, w_out]] = max_val;
                    max_indices[[b, c, w_out]] = max_idx;
                }
            }
        }

        // Cache max indices for backward pass
        *self.max_indices_cache.write().unwrap() = Some(max_indices);

        Ok(output)
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        let input_cache = self.input_cache.read().unwrap();
        let input = input_cache.as_ref().ok_or_else(|| {
            NeuralError::ComputationError("No cached input for backward pass".to_string())
        })?;

        let max_indices_cache = self.max_indices_cache.read().unwrap();
        let max_indices = max_indices_cache.as_ref().ok_or_else(|| {
            NeuralError::ComputationError("No cached max indices for backward pass".to_string())
        })?;

        let input_shape = input.shape();
        let batch_size = input_shape[0];
        let channels = input_shape[1];
        let output_width = self.output_size;

        let mut grad_input = Array::<F, IxDyn>::zeros(input.raw_dim());

        for b in 0..batch_size {
            for c in 0..channels {
                for w_out in 0..output_width {
                    let max_idx = max_indices[[b, c, w_out]];
                    grad_input[[b, c, max_idx]] =
                        grad_input[[b, c, max_idx]] + grad_output[[b, c, w_out]];
                }
            }
        }

        Ok(grad_input)
    }

    fn update(&mut self, _learning_rate: F) -> Result<()> {
        // AdaptiveMaxPool1D has no learnable parameters
        Ok(())
    }

    fn layer_type(&self) -> &str {
        "AdaptiveMaxPool1D"
    }

    fn parameter_count(&self) -> usize {
        // AdaptiveMaxPool1D has no parameters
        0
    }

    fn layer_description(&self) -> String {
        format!(
            "type:AdaptiveMaxPool1D, output_size:{}, name:{}",
            self.output_size,
            self.name.as_deref().unwrap_or("None")
        )
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

/// 3D Adaptive Average Pooling layer
///
/// Applies average pooling with adaptive output size for 3D data.
/// The output size is specified, and the pooling kernel size and stride are computed automatically.
pub struct AdaptiveAvgPool3D<F: Float + Debug + Send + Sync> {
    /// Output size (depth, height, width)
    output_size: (usize, usize, usize),
    /// Name of the layer
    name: Option<String>,
    /// Input cache for backward pass
    input_cache: Arc<RwLock<Option<Array<F, IxDyn>>>>,
    /// Phantom data for generic type
    _phantom: PhantomData<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> AdaptiveAvgPool3D<F> {
    /// Create a new adaptive average pooling layer for 3D data
    pub fn new(output_size: (usize, usize, usize), name: Option<&str>) -> Result<Self> {
        if output_size.0 == 0 || output_size.1 == 0 || output_size.2 == 0 {
            return Err(NeuralError::InvalidArchitecture(
                "Output size must be positive".to_string(),
            ));
        }

        Ok(Self {
            output_size,
            name: name.map(|s| s.to_string()),
            input_cache: Arc::new(RwLock::new(None)),
            _phantom: PhantomData,
        })
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> Layer<F> for AdaptiveAvgPool3D<F> {
    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        if input.ndim() != 5 {
            return Err(NeuralError::InvalidArgument(
                "Input must be 5D [batch_size, channels, depth, height, width]".to_string(),
            ));
        }

        let shape = input.shape();
        let batch_size = shape[0];
        let channels = shape[1];
        let input_depth = shape[2];
        let input_height = shape[3];
        let input_width = shape[4];

        // Cache input for backward pass
        *self.input_cache.write().unwrap() = Some(input.clone());

        let (output_depth, output_height, output_width) = self.output_size;

        // Create output array
        let mut output = Array::<F, IxDyn>::zeros(IxDyn(&[
            batch_size,
            channels,
            output_depth,
            output_height,
            output_width,
        ]));

        // Perform adaptive pooling
        for b in 0..batch_size {
            for c in 0..channels {
                for d_out in 0..output_depth {
                    for h_out in 0..output_height {
                        for w_out in 0..output_width {
                            let start_d = (d_out * input_depth) / output_depth;
                            let end_d = ((d_out + 1) * input_depth) / output_depth;
                            let start_h = (h_out * input_height) / output_height;
                            let end_h = ((h_out + 1) * input_height) / output_height;
                            let start_w = (w_out * input_width) / output_width;
                            let end_w = ((w_out + 1) * input_width) / output_width;

                            let mut sum = F::zero();
                            let mut count = F::zero();

                            for d in start_d..end_d {
                                for h in start_h..end_h {
                                    for w in start_w..end_w {
                                        sum = sum + input[[b, c, d, h, w]];
                                        count = count + F::one();
                                    }
                                }
                            }

                            output[[b, c, d_out, h_out, w_out]] = if count > F::zero() {
                                sum / count
                            } else {
                                F::zero()
                            };
                        }
                    }
                }
            }
        }

        Ok(output)
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        let input_cache = self.input_cache.read().unwrap();
        let input = input_cache.as_ref().ok_or_else(|| {
            NeuralError::ComputationError("No cached input for backward pass".to_string())
        })?;

        let input_shape = input.shape();
        let batch_size = input_shape[0];
        let channels = input_shape[1];
        let input_depth = input_shape[2];
        let input_height = input_shape[3];
        let input_width = input_shape[4];
        let (output_depth, output_height, output_width) = self.output_size;

        let mut grad_input = Array::<F, IxDyn>::zeros(input.raw_dim());

        for b in 0..batch_size {
            for c in 0..channels {
                for d_out in 0..output_depth {
                    for h_out in 0..output_height {
                        for w_out in 0..output_width {
                            let start_d = (d_out * input_depth) / output_depth;
                            let end_d = ((d_out + 1) * input_depth) / output_depth;
                            let start_h = (h_out * input_height) / output_height;
                            let end_h = ((h_out + 1) * input_height) / output_height;
                            let start_w = (w_out * input_width) / output_width;
                            let end_w = ((w_out + 1) * input_width) / output_width;

                            let pool_size =
                                F::from((end_d - start_d) * (end_h - start_h) * (end_w - start_w))
                                    .unwrap();
                            let grad_per_element =
                                grad_output[[b, c, d_out, h_out, w_out]] / pool_size;

                            for d in start_d..end_d {
                                for h in start_h..end_h {
                                    for w in start_w..end_w {
                                        grad_input[[b, c, d, h, w]] =
                                            grad_input[[b, c, d, h, w]] + grad_per_element;
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }

        Ok(grad_input)
    }

    fn update(&mut self, _learning_rate: F) -> Result<()> {
        // AdaptiveAvgPool3D has no learnable parameters
        Ok(())
    }

    fn layer_type(&self) -> &str {
        "AdaptiveAvgPool3D"
    }

    fn parameter_count(&self) -> usize {
        // AdaptiveAvgPool3D has no parameters
        0
    }

    fn layer_description(&self) -> String {
        format!(
            "type:AdaptiveAvgPool3D, output_size:({}, {}, {}), name:{}",
            self.output_size.0,
            self.output_size.1,
            self.output_size.2,
            self.name.as_deref().unwrap_or("None")
        )
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

/// 3D Adaptive Max Pooling layer
///
/// Applies max pooling with adaptive output size for 3D data.
/// The output size is specified, and the pooling kernel size and stride are computed automatically.
pub struct AdaptiveMaxPool3D<F: Float + Debug + Send + Sync> {
    /// Output size (depth, height, width)
    output_size: (usize, usize, usize),
    /// Name of the layer
    name: Option<String>,
    /// Input cache for backward pass
    input_cache: Arc<RwLock<Option<Array<F, IxDyn>>>>,
    /// Max indices cache for backward pass
    max_indices_cache: MaxIndicesCache3D,
    /// Phantom data for generic type
    _phantom: PhantomData<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> AdaptiveMaxPool3D<F> {
    /// Create a new adaptive max pooling layer for 3D data
    pub fn new(output_size: (usize, usize, usize), name: Option<&str>) -> Result<Self> {
        if output_size.0 == 0 || output_size.1 == 0 || output_size.2 == 0 {
            return Err(NeuralError::InvalidArchitecture(
                "Output size must be positive".to_string(),
            ));
        }

        Ok(Self {
            output_size,
            name: name.map(|s| s.to_string()),
            input_cache: Arc::new(RwLock::new(None)),
            max_indices_cache: Arc::new(RwLock::new(None)),
            _phantom: PhantomData,
        })
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + 'static> Layer<F> for AdaptiveMaxPool3D<F> {
    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        if input.ndim() != 5 {
            return Err(NeuralError::InvalidArgument(
                "Input must be 5D [batch_size, channels, depth, height, width]".to_string(),
            ));
        }

        let shape = input.shape();
        let batch_size = shape[0];
        let channels = shape[1];
        let input_depth = shape[2];
        let input_height = shape[3];
        let input_width = shape[4];

        // Cache input for backward pass
        *self.input_cache.write().unwrap() = Some(input.clone());

        let (output_depth, output_height, output_width) = self.output_size;

        // Create output array and indices array
        let mut output = Array::<F, IxDyn>::zeros(IxDyn(&[
            batch_size,
            channels,
            output_depth,
            output_height,
            output_width,
        ]));
        let mut max_indices = Array::<(usize, usize, usize), IxDyn>::from_elem(
            IxDyn(&[
                batch_size,
                channels,
                output_depth,
                output_height,
                output_width,
            ]),
            (0, 0, 0),
        );

        // Perform adaptive pooling
        for b in 0..batch_size {
            for c in 0..channels {
                for d_out in 0..output_depth {
                    for h_out in 0..output_height {
                        for w_out in 0..output_width {
                            let start_d = (d_out * input_depth) / output_depth;
                            let end_d = ((d_out + 1) * input_depth) / output_depth;
                            let start_h = (h_out * input_height) / output_height;
                            let end_h = ((h_out + 1) * input_height) / output_height;
                            let start_w = (w_out * input_width) / output_width;
                            let end_w = ((w_out + 1) * input_width) / output_width;

                            let mut max_val = F::neg_infinity();
                            let mut max_idx = (start_d, start_h, start_w);

                            for d in start_d..end_d {
                                for h in start_h..end_h {
                                    for w in start_w..end_w {
                                        let val = input[[b, c, d, h, w]];
                                        if val > max_val {
                                            max_val = val;
                                            max_idx = (d, h, w);
                                        }
                                    }
                                }
                            }

                            output[[b, c, d_out, h_out, w_out]] = max_val;
                            max_indices[[b, c, d_out, h_out, w_out]] = max_idx;
                        }
                    }
                }
            }
        }

        // Cache max indices for backward pass
        *self.max_indices_cache.write().unwrap() = Some(max_indices);

        Ok(output)
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        let input_cache = self.input_cache.read().unwrap();
        let input = input_cache.as_ref().ok_or_else(|| {
            NeuralError::ComputationError("No cached input for backward pass".to_string())
        })?;

        let max_indices_cache = self.max_indices_cache.read().unwrap();
        let max_indices = max_indices_cache.as_ref().ok_or_else(|| {
            NeuralError::ComputationError("No cached max indices for backward pass".to_string())
        })?;

        let input_shape = input.shape();
        let batch_size = input_shape[0];
        let channels = input_shape[1];
        let (output_depth, output_height, output_width) = self.output_size;

        let mut grad_input = Array::<F, IxDyn>::zeros(input.raw_dim());

        for b in 0..batch_size {
            for c in 0..channels {
                for d_out in 0..output_depth {
                    for h_out in 0..output_height {
                        for w_out in 0..output_width {
                            let (max_d, max_h, max_w) = max_indices[[b, c, d_out, h_out, w_out]];
                            grad_input[[b, c, max_d, max_h, max_w]] = grad_input
                                [[b, c, max_d, max_h, max_w]]
                                + grad_output[[b, c, d_out, h_out, w_out]];
                        }
                    }
                }
            }
        }

        Ok(grad_input)
    }

    fn update(&mut self, _learning_rate: F) -> Result<()> {
        // AdaptiveMaxPool3D has no learnable parameters
        Ok(())
    }

    fn layer_type(&self) -> &str {
        "AdaptiveMaxPool3D"
    }

    fn parameter_count(&self) -> usize {
        // AdaptiveMaxPool3D has no parameters
        0
    }

    fn layer_description(&self) -> String {
        format!(
            "type:AdaptiveMaxPool3D, output_size:({}, {}, {}), name:{}",
            self.output_size.0,
            self.output_size.1,
            self.output_size.2,
            self.name.as_deref().unwrap_or("None")
        )
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}