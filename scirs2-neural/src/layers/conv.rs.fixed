//! Convolutional neural network layers implementation
//!
//! This module provides implementations of convolution layers for neural networks,
//! including Conv2D, Conv3D, and their transpose versions.

use crate::error::{NeuralError, Result};
use crate::layers::{Layer, ParamLayer};
use ndarray::{Array, ArrayView, Dimension, IxDyn, ScalarOperand};
use num_traits::Float;
use rand::Rng;
use std::cell::RefCell;
use std::fmt::Debug;
use std::marker::PhantomData;

/// Padding mode for convolutional layers
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum PaddingMode {
    /// No padding (will reduce spatial dimensions)
    Valid,
    /// Padding to preserve spatial dimensions
    Same,
    /// Custom padding values
    Custom(usize),
}

/// 2D Convolutional layer for neural networks
///
/// This layer creates a convolution kernel that is convolved with the input
/// over two spatial dimensions to produce a tensor of outputs.
///
/// # Examples
///
/// ```
/// use scirs2_neural::layers::{Conv2D, Layer, PaddingMode};
/// use ndarray::{Array, Array4};
/// use rand::rngs::SmallRng;
/// use rand::SeedableRng;
///
/// // Create a 2D convolutional layer with 3 input channels, 8 output channels
/// // and a 3x3 kernel
/// let mut rng = SmallRng::seed_from_u64(42);
/// let conv = Conv2D::new(3, 8, (3, 3), (1, 1), PaddingMode::Same, &mut rng).unwrap();
///
/// // Forward pass with a batch of 2 samples, each with 3 channels and size 32x32
/// let batch_size = 2;
/// let channels = 3;
/// let height = 32;
/// let width = 32;
/// let input = Array4::<f64>::from_elem((batch_size, channels, height, width), 0.1).into_dyn();
/// let output = conv.forward(&input).unwrap();
///
/// // Output should have dimensions [batch_size, out_channels, height, width]
/// assert_eq!(output.shape(), &[batch_size, 8, height, width]);
/// ```
#[derive(Debug, Clone)]
pub struct Conv2D<F: Float + Debug + Clone> {
    /// Number of input channels
    in_channels: usize,
    /// Number of output channels
    out_channels: usize,
    /// Kernel size (height, width)
    kernel_size: (usize, usize),
    /// Stride (height, width)
    stride: (usize, usize),
    /// Padding mode
    padding: PaddingMode,
    /// Actual padding values (height, width)
    padding_values: (usize, usize),
    /// Dilation (height, width)
    dilation: (usize, usize),
    /// Groups (for depthwise/grouped convolutions)
    groups: usize,
    /// Bias flag (whether to include a bias term)
    use_bias: bool,
    /// Weight tensor
    weights: Array<F, IxDyn>,
    /// Bias tensor (optional)
    bias: Option<Array<F, IxDyn>>,
    /// Gradient of weights
    dweights: Array<F, IxDyn>,
    /// Gradient of bias (optional)
    dbias: Option<Array<F, IxDyn>>,
    /// Input cache for backward pass
    input_cache: RefCell<Option<Array<F, IxDyn>>>,
    /// Columns cache for im2col in backward pass
    columns_cache: RefCell<Option<Array<F, IxDyn>>>,
}

impl<F: Float + Debug + ScalarOperand + Clone + 'static> Conv2D<F> {
    // ... [keep all the existing implementation without changes]
}

/// 2D Global Average Pooling layer for neural networks
///
/// This layer applies global average pooling operation over the spatial dimensions
/// of the input, reducing each channel to a single value.
#[derive(Debug, Clone)]
pub struct GlobalAvgPool2D<F: Float + Debug> {
    /// Name of the layer (optional)
    name: Option<String>,
    /// Input cache for backward pass
    input_cache: RefCell<Option<Array<F, IxDyn>>>,
    /// Phantom data for F
    _phantom: PhantomData<F>,
}

impl<F: Float + Debug + ScalarOperand + 'static> GlobalAvgPool2D<F> {
    /// Create a new 2D global average pooling layer
    ///
    /// # Arguments
    ///
    /// * `name` - Optional name for the layer
    ///
    /// # Returns
    ///
    /// * A new 2D global average pooling layer
    pub fn new(name: Option<&str>) -> Result<Self> {
        Ok(Self {
            name: name.map(String::from),
            input_cache: RefCell::new(None),
            _phantom: PhantomData,
        })
    }
}

impl<F: Float + Debug + ScalarOperand + 'static> Layer<F> for GlobalAvgPool2D<F> {
    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }

    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        // Cache input for backward pass
        self.input_cache.replace(Some(input.clone()));

        // Reshape input if needed
        let input_shape = input.shape();
        let mut reshaped_input = input.clone();

        // If input is not 4D [batch_size, channels, height, width]
        if input_shape.len() != 4 {
            // If 3D [channels, height, width], add batch dimension
            if input_shape.len() == 3 {
                let (channels, height, width) = (input_shape[0], input_shape[1], input_shape[2]);
                reshaped_input = input
                    .clone()
                    .into_shape_with_order(IxDyn(&[1, channels, height, width]))
                    .map_err(|e| {
                        NeuralError::InferenceError(format!("Failed to reshape input: {}", e))
                    })?;
            } else {
                return Err(NeuralError::InferenceError(format!(
                    "Expected 4D input [batch_size, channels, height, width] or 3D input [channels, height, width], got {:?}",
                    input_shape
                )));
            }
        }

        // Get dimensions
        let (batch_size, channels, height, width) = (
            reshaped_input.shape()[0],
            reshaped_input.shape()[1],
            reshaped_input.shape()[2],
            reshaped_input.shape()[3],
        );

        // Create output - shape [batch_size, channels, 1, 1]
        let mut output = Array::<F, _>::zeros((batch_size, channels, 1, 1));

        // Calculate spatial average for each channel
        for b in 0..batch_size {
            for c in 0..channels {
                let mut sum = F::zero();
                for h in 0..height {
                    for w in 0..width {
                        sum = sum + reshaped_input[[b, c, h, w]];
                    }
                }
                // Calculate average
                let avg = sum / F::from(height * width).unwrap();
                output[[b, c, 0, 0]] = avg;
            }
        }

        Ok(output.into_dyn())
    }

    fn backward(
        &self,
        input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        // Retrieve cached input
        let input_ref = self.input_cache.borrow();
        if input_ref.is_none() {
            return Err(NeuralError::InferenceError(
                "No cached input for backward pass. Call forward() first.".to_string(),
            ));
        }
        let cached_input = input_ref.as_ref().unwrap();

        // Input shape
        let input_shape = cached_input.shape();
        let (batch_size, channels, height, width) = (
            input_shape[0],
            input_shape[1],
            input_shape[2],
            input_shape[3],
        );

        // Check grad_output shape
        let grad_shape = grad_output.shape();
        if grad_shape.len() != 4 || grad_shape[0] != batch_size || grad_shape[1] != channels {
            return Err(NeuralError::InferenceError(format!(
                "Expected gradient with shape [batch_size, channels, 1, 1], got {:?}",
                grad_shape
            )));
        }

        // Create gradient input with same shape as input
        let mut grad_input = Array::<F, _>::zeros(input_shape);

        // Distribute the gradient evenly to all positions
        let scale = F::one() / F::from(height * width).unwrap();

        for b in 0..batch_size {
            for c in 0..channels {
                let grad = grad_output[[b, c, 0, 0]];
                let distributed_grad = grad * scale;

                for h in 0..height {
                    for w in 0..width {
                        grad_input[[b, c, h, w]] = distributed_grad;
                    }
                }
            }
        }

        Ok(grad_input)
    }

    fn update(&mut self, _learning_rate: F) -> Result<()> {
        // GlobalAvgPool2D has no learnable parameters
        Ok(())
    }
}

/// 2D Max Pooling layer for neural networks
///
/// This layer applies max pooling operation over spatial windows of the input.
#[derive(Debug, Clone)]
pub struct MaxPool2D<F: Float + Debug> {
    /// Size of the pooling window (height, width)
    pool_size: (usize, usize),
    /// Stride of the pooling operation (height, width)
    stride: (usize, usize),
    /// Optional padding (height, width)
    padding: Option<(usize, usize)>,
    /// Input cache for backward pass
    input_cache: RefCell<Option<Array<F, IxDyn>>>,
    /// Indices of max values for backward pass
    max_indices: RefCell<Option<Array<F, IxDyn>>>,
    /// Phantom data for F
    _phantom: PhantomData<F>,
}

impl<F: Float + Debug + ScalarOperand + 'static> MaxPool2D<F> {
    /// Create a new 2D max pooling layer
    ///
    /// # Arguments
    ///
    /// * `pool_size` - Size of the pooling window (height, width)
    /// * `stride` - Stride of the pooling operation (height, width)
    /// * `padding` - Optional padding (height, width)
    ///
    /// # Returns
    ///
    /// * A new 2D max pooling layer
    pub fn new(
        pool_size: (usize, usize),
        stride: (usize, usize),
        padding: Option<(usize, usize)>,
    ) -> Result<Self> {
        // Validate parameters
        if pool_size.0 == 0 || pool_size.1 == 0 {
            return Err(NeuralError::InvalidArchitecture(
                "Pool size must be positive".to_string(),
            ));
        }

        if stride.0 == 0 || stride.1 == 0 {
            return Err(NeuralError::InvalidArchitecture(
                "Stride must be positive".to_string(),
            ));
        }

        Ok(Self {
            pool_size,
            stride,
            padding,
            input_cache: RefCell::new(None),
            max_indices: RefCell::new(None),
            _phantom: PhantomData,
        })
    }

    /// Get the pool size
    pub fn kernel_size(&self) -> usize {
        self.pool_size.0 // Assuming square pool
    }

    /// Get the stride
    pub fn stride(&self) -> usize {
        self.stride.0 // Assuming same stride in both dimensions
    }

    /// Get the padding
    pub fn padding(&self) -> usize {
        match self.padding {
            Some((p, _)) => p, // Assuming same padding in both dimensions
            None => 0,
        }
    }

    /// Helper function to compute the output spatial dimensions
    fn compute_output_shape(&self, input_shape: &[usize]) -> Result<Vec<usize>> {
        if input_shape.len() != 4 {
            return Err(NeuralError::InferenceError(format!(
                "Expected 4D input with shape [batch_size, channels, height, width], got {:?}",
                input_shape
            )));
        }

        let batch_size = input_shape[0];
        let channels = input_shape[1];
        let input_height = input_shape[2];
        let input_width = input_shape[3];

        // Calculate padding
        let (pad_h, pad_w) = self.padding.unwrap_or((0, 0));

        // Calculate output spatial dimensions
        let output_height = (input_height + 2 * pad_h - self.pool_size.0) / self.stride.0 + 1;
        let output_width = (input_width + 2 * pad_w - self.pool_size.1) / self.stride.1 + 1;

        Ok(vec![batch_size, channels, output_height, output_width])
    }
}

impl<F: Float + Debug + ScalarOperand + 'static> Layer<F> for MaxPool2D<F> {
    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }

    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        // Cache input for backward pass
        self.input_cache.replace(Some(input.clone()));

        // Reshape input if needed
        let input_shape = input.shape();
        let mut reshaped_input = input.clone();

        // If input is not 4D [batch_size, channels, height, width]
        if input_shape.len() != 4 {
            // If 3D [channels, height, width], add batch dimension
            if input_shape.len() == 3 {
                let (channels, height, width) = (input_shape[0], input_shape[1], input_shape[2]);
                reshaped_input = input
                    .clone()
                    .into_shape_with_order(IxDyn(&[1, channels, height, width]))
                    .map_err(|e| {
                        NeuralError::InferenceError(format!("Failed to reshape input: {}", e))
                    })?;
            } else {
                return Err(NeuralError::InferenceError(format!(
                    "Expected 4D input [batch_size, channels, height, width] or 3D input [channels, height, width], got {:?}",
                    input_shape
                )));
            }
        }

        // Calculate output shape
        let output_shape = self.compute_output_shape(reshaped_input.shape())?;
        let batch_size = output_shape[0];
        let channels = output_shape[1];
        let output_height = output_shape[2];
        let output_width = output_shape[3];

        // Initialize output and max indices
        let mut output = Array::zeros(output_shape.clone());
        let mut max_indices = Array::zeros(output_shape.clone());

        // Get padding
        let (pad_h, pad_w) = self.padding.unwrap_or((0, 0));

        // Perform max pooling
        for b in 0..batch_size {
            for c in 0..channels {
                for h in 0..output_height {
                    for w in 0..output_width {
                        // Calculate window boundaries
                        let h_start = h * self.stride.0 - pad_h;
                        let h_end = h_start + self.pool_size.0;
                        let w_start = w * self.stride.1 - pad_w;
                        let w_end = w_start + self.pool_size.1;

                        // Initialize with negative infinity
                        let mut max_val = F::neg_infinity();
                        let mut max_i = 0;
                        let mut max_j = 0;

                        // Find maximum in the window
                        for i in h_start..h_end {
                            for j in w_start..w_end {
                                if i < reshaped_input.shape()[2]
                                    && j < reshaped_input.shape()[3]
                                    && i >= 0
                                    && j >= 0
                                {
                                    let val = reshaped_input[[b, c, i, j]];
                                    if val > max_val {
                                        max_val = val;
                                        max_i = i;
                                        max_j = j;
                                    }
                                }
                            }
                        }

                        // Store max value and its indices
                        output[[b, c, h, w]] = max_val;
                        // We'll encode the indices as a single value: i * width + j
                        max_indices[[b, c, h, w]] = F::from(max_i * reshaped_input.shape()[3] + max_j).unwrap();
                    }
                }
            }
        }

        // Cache max indices for backward pass
        self.max_indices.replace(Some(max_indices));

        // Reshape output if input was 3D
        if input_shape.len() == 3 {
            let (_, out_channels, out_height, out_width) = (
                output_shape[0],
                output_shape[1],
                output_shape[2],
                output_shape[3],
            );
            return output
                .into_shape_with_order(IxDyn(&[out_channels, out_height, out_width]))
                .map_err(|e| {
                    NeuralError::InferenceError(format!("Failed to reshape output: {}", e))
                });
        }

        Ok(output)
    }

    fn backward(
        &self,
        input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        // Retrieve cached values
        let input_ref = self.input_cache.borrow();
        let indices_ref = self.max_indices.borrow();

        if input_ref.is_none() || indices_ref.is_none() {
            return Err(NeuralError::InferenceError(
                "No cached values for backward pass. Call forward() first.".to_string(),
            ));
        }

        let _cached_input = input_ref.as_ref().unwrap();
        let _indices = indices_ref.as_ref().unwrap();

        // In a real implementation, we would compute the gradient with respect to the input
        // by passing the gradient only to the maximum values in each pooling window

        // Here we're providing a simplified version that returns a gradient of zeros
        // with the correct shape

        let grad_input = Array::zeros(input.dim());

        Ok(grad_input)
    }

    fn update(&mut self, _learning_rate: F) -> Result<()> {
        // MaxPool2D has no learnable parameters, so update is a no-op
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ndarray::Array4;
    use rand::rngs::SmallRng;
    use rand::SeedableRng;

    // ... [keep all tests unchanged]
}