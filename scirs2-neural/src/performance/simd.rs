//! SIMD-accelerated operations for neural networks
//!
//! This module provides vectorized implementations of common neural network operations
//! using SIMD (Single Instruction, Multiple Data) instructions for significant
//! performance improvements. All functions are feature-gated with "simd" feature.

use crate::error::{NeuralError, Result};
use ndarray::{Array, ArrayD};
#[cfg(feature = "simd")]
use ndarray::{ArrayView, ArrayViewMut, IxDyn};
#[allow(unused_imports)]
use num_traits::Float;
use std::fmt::Debug;

#[cfg(feature = "simd")]
use wide::{f32x8, CmpGt};

/// SIMD-accelerated operations for neural networks
#[cfg(feature = "simd")]
pub struct SIMDOperations;

#[cfg(feature = "simd")]
impl SIMDOperations {
    /// SIMD-accelerated ReLU activation for f32 arrays
    pub fn simd_relu_f32_inplace(input: &mut ArrayViewMut<f32, IxDyn>) {
        if let Some(slice) = input.as_slice_mut() {
            Self::simd_relu_f32_slice(slice);
        } else {
            input.mapv_inplace(|x| x.max(0.0));
        }
    }

    /// SIMD-accelerated ReLU for f32 slice
    fn simd_relu_f32_slice(slice: &mut [f32]) {
        let mut i = 0;
        let chunk_size = 8;
        let zero = f32x8::splat(0.0);

        while i + chunk_size <= slice.len() {
            let values = [
                slice[i],
                slice[i + 1],
                slice[i + 2],
                slice[i + 3],
                slice[i + 4],
                slice[i + 5],
                slice[i + 6],
                slice[i + 7],
            ];
            let vec = f32x8::new(values);
            let result = vec.cmp_gt(zero).blend(vec, zero);
            let result_arr: [f32; 8] = result.into();
            slice[i..i + chunk_size].copy_from_slice(&result_arr);
            i += chunk_size;
        }

        // Process remaining elements
        for val in &mut slice[i..] {
            *val = val.max(0.0);
        }
    }

    /// Vectorized ReLU activation returning new array
    pub fn simd_relu_f32(input: &ArrayView<f32, IxDyn>) -> ArrayD<f32> {
        let mut result = input.to_owned();
        Self::simd_relu_f32_inplace(&mut result.view_mut());
        result
    }

    /// SIMD-accelerated sigmoid activation for f32 arrays
    pub fn simd_sigmoid_f32_inplace(input: &mut ArrayViewMut<f32, IxDyn>) {
        if let Some(slice) = input.as_slice_mut() {
            Self::simd_sigmoid_f32_slice(slice);
        } else {
            input.mapv_inplace(|x| 1.0 / (1.0 + (-x).exp()));
        }
    }

    /// SIMD-accelerated sigmoid for f32 slice
    fn simd_sigmoid_f32_slice(slice: &mut [f32]) {
        let mut i = 0;
        let chunk_size = 8;
        let one = f32x8::splat(1.0);

        while i + chunk_size <= slice.len() {
            let values = [
                slice[i],
                slice[i + 1],
                slice[i + 2],
                slice[i + 3],
                slice[i + 4],
                slice[i + 5],
                slice[i + 6],
                slice[i + 7],
            ];
            let vec = f32x8::new(values);
            let neg_vec = -vec;
            let exp_vec = neg_vec.exp();
            let result = one / (one + exp_vec);
            let result_arr: [f32; 8] = result.into();
            slice[i..i + chunk_size].copy_from_slice(&result_arr);
            i += chunk_size;
        }

        // Process remaining elements
        for val in &mut slice[i..] {
            *val = 1.0 / (1.0 + (-*val).exp());
        }
    }

    /// Vectorized sigmoid activation returning new array
    pub fn simd_sigmoid_f32(input: &ArrayView<f32, IxDyn>) -> ArrayD<f32> {
        let mut result = input.to_owned();
        Self::simd_sigmoid_f32_inplace(&mut result.view_mut());
        result
    }

    /// SIMD-accelerated tanh activation for f32 arrays
    pub fn simd_tanh_f32_inplace(input: &mut ArrayViewMut<f32, IxDyn>) {
        if let Some(slice) = input.as_slice_mut() {
            Self::simd_tanh_f32_slice(slice);
        } else {
            input.mapv_inplace(|x| x.tanh());
        }
    }

    /// SIMD-accelerated tanh for f32 slice
    fn simd_tanh_f32_slice(slice: &mut [f32]) {
        let mut i = 0;
        let chunk_size = 8;

        while i + chunk_size <= slice.len() {
            let values = [
                slice[i],
                slice[i + 1],
                slice[i + 2],
                slice[i + 3],
                slice[i + 4],
                slice[i + 5],
                slice[i + 6],
                slice[i + 7],
            ];
            let vec = f32x8::new(values);
            // Manual tanh implementation: tanh(x) = (e^x - e^-x) / (e^x + e^-x)
            let pos_exp = vec.exp();
            let neg_exp = (-vec).exp();
            let result = (pos_exp - neg_exp) / (pos_exp + neg_exp);
            let result_arr: [f32; 8] = result.into();
            slice[i..i + chunk_size].copy_from_slice(&result_arr);
            i += chunk_size;
        }

        // Process remaining elements
        for val in &mut slice[i..] {
            *val = val.tanh();
        }
    }

    /// Vectorized tanh activation returning new array
    pub fn simd_tanh_f32(input: &ArrayView<f32, IxDyn>) -> ArrayD<f32> {
        let mut result = input.to_owned();
        Self::simd_tanh_f32_inplace(&mut result.view_mut());
        result
    }

    /// SIMD-accelerated GELU activation for f32 arrays
    pub fn simd_gelu_f32(input: &ArrayView<f32, IxDyn>) -> ArrayD<f32> {
        let mut result = input.to_owned();
        if let Some(slice) = result.as_slice_mut() {
            Self::simd_gelu_f32_slice(slice);
        } else {
            result.mapv_inplace(|x| {
                0.5 * x * (1.0 + (x * 0.7978845608 * (1.0 + 0.044715 * x * x)).tanh())
            });
        }
        result
    }

    /// SIMD-accelerated GELU for f32 slice (approximation)
    fn simd_gelu_f32_slice(slice: &mut [f32]) {
        let mut i = 0;
        let chunk_size = 8;
        let half = f32x8::splat(0.5);
        let one = f32x8::splat(1.0);
        let coeff1 = f32x8::splat(0.7978845608); // sqrt(2/Ï€)
        let coeff2 = f32x8::splat(0.044715);

        while i + chunk_size <= slice.len() {
            let values = [
                slice[i],
                slice[i + 1],
                slice[i + 2],
                slice[i + 3],
                slice[i + 4],
                slice[i + 5],
                slice[i + 6],
                slice[i + 7],
            ];
            let x = f32x8::new(values);
            let x_sq = x * x;
            let x_cube = x * x_sq;
            let inner = x * coeff1 * (one + coeff2 * x_cube);
            // Manual tanh implementation for GELU: tanh(x) = (e^x - e^-x) / (e^x + e^-x)
            let pos_exp = inner.exp();
            let neg_exp = (-inner).exp();
            let tanh_val = (pos_exp - neg_exp) / (pos_exp + neg_exp);
            let result = half * x * (one + tanh_val);
            let result_arr: [f32; 8] = result.into();
            slice[i..i + chunk_size].copy_from_slice(&result_arr);
            i += chunk_size;
        }

        // Process remaining elements
        for val in &mut slice[i..] {
            let x = *val;
            *val = 0.5 * x * (1.0 + (x * 0.7978845608 * (1.0 + 0.044715 * x * x)).tanh());
        }
    }

    /// SIMD-accelerated Swish/SiLU activation for f32 arrays
    pub fn simd_swish_f32(input: &ArrayView<f32, IxDyn>) -> ArrayD<f32> {
        let mut result = input.to_owned();
        if let Some(slice) = result.as_slice_mut() {
            Self::simd_swish_f32_slice(slice);
        } else {
            result.mapv_inplace(|x| x / (1.0 + (-x).exp()));
        }
        result
    }

    /// SIMD-accelerated Swish for f32 slice
    fn simd_swish_f32_slice(slice: &mut [f32]) {
        let mut i = 0;
        let chunk_size = 8;
        let one = f32x8::splat(1.0);

        while i + chunk_size <= slice.len() {
            let values = [
                slice[i],
                slice[i + 1],
                slice[i + 2],
                slice[i + 3],
                slice[i + 4],
                slice[i + 5],
                slice[i + 6],
                slice[i + 7],
            ];
            let x = f32x8::new(values);
            let neg_x = -x;
            let exp_neg_x = neg_x.exp();
            let sigmoid = one / (one + exp_neg_x);
            let result = x * sigmoid;
            let result_arr: [f32; 8] = result.into();
            slice[i..i + chunk_size].copy_from_slice(&result_arr);
            i += chunk_size;
        }

        // Process remaining elements
        for val in &mut slice[i..] {
            let x = *val;
            *val = x / (1.0 + (-x).exp());
        }
    }

    /// SIMD-accelerated softmax for f32 arrays
    pub fn simd_softmax_f32(input: &ArrayD<f32>, axis: Option<usize>) -> Result<ArrayD<f32>> {
        let axis = axis.unwrap_or(input.ndim() - 1);

        if axis >= input.ndim() {
            return Err(NeuralError::InvalidArchitecture(
                "Softmax axis out of bounds".to_string(),
            ));
        }

        let mut result = Array::zeros(input.raw_dim());

        // Process along the specified axis
        for lane in input.axis_iter(ndarray::Axis(axis)) {
            if let (Some(input_slice), Some(mut result_slice)) = (
                lane.as_slice(),
                result.axis_iter_mut(ndarray::Axis(axis)).next(),
            ) {
                if let Some(result_slice_mut) = result_slice.as_slice_mut() {
                    Self::simd_softmax_f32_slice(input_slice, result_slice_mut);
                }
            }
        }

        Ok(result)
    }

    /// SIMD softmax for f32 slice
    fn simd_softmax_f32_slice(input: &[f32], result: &mut [f32]) {
        if input.is_empty() {
            return;
        }

        // Find maximum for numerical stability
        let max_val = input.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b));

        // Compute exp(x - max) using SIMD
        let mut i = 0;
        let chunk_size = 8;
        let max_vec = f32x8::splat(max_val);
        let mut sum = 0.0f32;

        // First pass: compute exp(x - max) and accumulate sum
        while i + chunk_size <= input.len() {
            let input_vals = [
                input[i],
                input[i + 1],
                input[i + 2],
                input[i + 3],
                input[i + 4],
                input[i + 5],
                input[i + 6],
                input[i + 7],
            ];
            let input_vec = f32x8::new(input_vals);
            let shifted = input_vec - max_vec;
            let exp_vec = shifted.exp();
            let exp_arr: [f32; 8] = exp_vec.into();

            for (j, &val) in exp_arr.iter().enumerate() {
                result[i + j] = val;
                sum += val;
            }
            i += chunk_size;
        }

        // Process remaining elements
        for idx in i..input.len() {
            let exp_val = (input[idx] - max_val).exp();
            result[idx] = exp_val;
            sum += exp_val;
        }

        // Second pass: divide by sum using SIMD
        let sum_vec = f32x8::splat(sum);
        i = 0;

        while i + chunk_size <= result.len() {
            let exp_vals = [
                result[i],
                result[i + 1],
                result[i + 2],
                result[i + 3],
                result[i + 4],
                result[i + 5],
                result[i + 6],
                result[i + 7],
            ];
            let exp_vec = f32x8::new(exp_vals);
            let softmax_vec = exp_vec / sum_vec;
            let softmax_arr: [f32; 8] = softmax_vec.into();
            result[i..i + chunk_size].copy_from_slice(&softmax_arr);
            i += chunk_size;
        }

        // Process remaining elements
        for val in &mut result[i..] {
            *val /= sum;
        }
    }

    /// SIMD-accelerated cross-entropy loss computation
    pub fn simd_cross_entropy_loss_f32(
        predictions: &ArrayView<f32, IxDyn>,
        targets: &ArrayView<f32, IxDyn>,
        epsilon: f32,
    ) -> Result<f32> {
        if predictions.shape() != targets.shape() {
            return Err(NeuralError::ComputationError(
                "Predictions and targets must have the same shape".to_string(),
            ));
        }

        if let (Some(pred_slice), Some(target_slice)) = (predictions.as_slice(), targets.as_slice())
        {
            Ok(Self::simd_cross_entropy_f32_slices(
                pred_slice,
                target_slice,
                epsilon,
            ))
        } else {
            // Fallback
            let mut loss = 0.0;
            for (&pred, &target) in predictions.iter().zip(targets.iter()) {
                let clamped_pred = pred.max(epsilon).min(1.0 - epsilon);
                loss -= target * clamped_pred.ln();
            }
            Ok(loss / predictions.len() as f32)
        }
    }

    /// SIMD cross-entropy for f32 slices
    fn simd_cross_entropy_f32_slices(predictions: &[f32], targets: &[f32], epsilon: f32) -> f32 {
        let mut i = 0;
        let chunk_size = 8;
        let eps_vec = f32x8::splat(epsilon);
        let one_minus_eps = f32x8::splat(1.0 - epsilon);
        let mut loss_vec = f32x8::splat(0.0);
        let len = predictions.len().min(targets.len());

        while i + chunk_size <= len {
            let pred_vals = [
                predictions[i],
                predictions[i + 1],
                predictions[i + 2],
                predictions[i + 3],
                predictions[i + 4],
                predictions[i + 5],
                predictions[i + 6],
                predictions[i + 7],
            ];
            let target_vals = [
                targets[i],
                targets[i + 1],
                targets[i + 2],
                targets[i + 3],
                targets[i + 4],
                targets[i + 5],
                targets[i + 6],
                targets[i + 7],
            ];

            let pred_vec = f32x8::new(pred_vals);
            let target_vec = f32x8::new(target_vals);

            // Clamp predictions to [epsilon, 1-epsilon]
            let clamped_pred = pred_vec.max(eps_vec).min(one_minus_eps);
            let log_pred = clamped_pred.ln();
            let loss_chunk = -(target_vec * log_pred);
            loss_vec = loss_vec + loss_chunk;

            i += chunk_size;
        }

        // Sum the SIMD loss vector
        let loss_arr: [f32; 8] = loss_vec.into();
        let mut total_loss: f32 = loss_arr.iter().sum();

        // Process remaining elements
        for idx in i..len {
            let clamped_pred = predictions[idx].max(epsilon).min(1.0 - epsilon);
            total_loss -= targets[idx] * clamped_pred.ln();
        }

        total_loss / len as f32
    }

    /// SIMD-accelerated matrix multiplication
    pub fn simd_matmul_f32(
        a: &ArrayView<f32, IxDyn>,
        b: &ArrayView<f32, IxDyn>,
    ) -> Result<ArrayD<f32>> {
        if a.ndim() != 2 || b.ndim() != 2 {
            return Err(NeuralError::ComputationError(
                "SIMD matmul only supports 2D arrays".to_string(),
            ));
        }

        let (m, k) = (a.shape()[0], a.shape()[1]);
        let (k2, n) = (b.shape()[0], b.shape()[1]);

        if k != k2 {
            return Err(NeuralError::ComputationError(
                "Matrix dimensions don't match for multiplication".to_string(),
            ));
        }

        let mut result = Array::zeros((m, n));

        // Use SIMD for inner loop vectorization
        for i in 0..m {
            for j in 0..n {
                let mut sum = 0.0f32;

                // SIMD-accelerated dot product
                if let (Some(a_row), Some(b_col)) = (
                    a.row(i).as_slice(),
                    // For column access, we need to extract values manually
                    None::<&[f32]>, // b column access is complex for SIMD
                ) {
                    if let Some(a_slice) = a_row {
                        sum = Self::simd_dot_product_f32(a_slice, &Self::extract_column(b, j));
                    }
                } else {
                    // Fallback to standard computation
                    for l in 0..k {
                        sum += a[[i, l]] * b[[l, j]];
                    }
                }

                result[[i, j]] = sum;
            }
        }

        Ok(result.into_dyn())
    }

    /// Extract column from 2D array for SIMD operations
    fn extract_column(matrix: &ArrayView<f32, IxDyn>, col_idx: usize) -> Vec<f32> {
        let rows = matrix.shape()[0];
        let mut column = Vec::with_capacity(rows);
        for i in 0..rows {
            column.push(matrix[[i, col_idx]]);
        }
        column
    }

    /// SIMD-accelerated dot product
    fn simd_dot_product_f32(a: &[f32], b: &[f32]) -> f32 {
        let mut i = 0;
        let chunk_size = 8;
        let mut sum_vec = f32x8::splat(0.0);
        let len = a.len().min(b.len());

        while i + chunk_size <= len {
            let a_vals = [
                a[i], a[i + 1], a[i + 2], a[i + 3], a[i + 4], a[i + 5], a[i + 6], a[i + 7],
            ];
            let b_vals = [
                b[i], b[i + 1], b[i + 2], b[i + 3], b[i + 4], b[i + 5], b[i + 6], b[i + 7],
            ];

            let a_vec = f32x8::new(a_vals);
            let b_vec = f32x8::new(b_vals);
            let product = a_vec * b_vec;
            sum_vec = sum_vec + product;

            i += chunk_size;
        }

        // Sum the SIMD vector
        let sum_arr: [f32; 8] = sum_vec.into();
        let mut total: f32 = sum_arr.iter().sum();

        // Process remaining elements
        for idx in i..len {
            total += a[idx] * b[idx];
        }

        total
    }

    /// SIMD-accelerated element-wise addition
    pub fn simd_add_f32(
        a: &ArrayView<f32, IxDyn>,
        b: &ArrayView<f32, IxDyn>,
    ) -> Result<ArrayD<f32>> {
        if a.shape() != b.shape() {
            return Err(NeuralError::ComputationError(
                "Arrays must have the same shape for addition".to_string(),
            ));
        }

        let mut result = Array::zeros(a.raw_dim());

        if let (Some(a_slice), Some(b_slice), Some(result_slice)) = (
            a.as_slice(),
            b.as_slice(),
            result.as_slice_mut(),
        ) {
            Self::simd_add_f32_slices(a_slice, b_slice, result_slice);
        } else {
            // Fallback for non-contiguous arrays
            for ((a_val, b_val), result_val) in a.iter().zip(b.iter()).zip(result.iter_mut()) {
                *result_val = a_val + b_val;
            }
        }

        Ok(result)
    }

    /// SIMD element-wise addition for slices
    fn simd_add_f32_slices(a: &[f32], b: &[f32], result: &mut [f32]) {
        let mut i = 0;
        let chunk_size = 8;
        let len = a.len().min(b.len()).min(result.len());

        while i + chunk_size <= len {
            let a_vals = [
                a[i], a[i + 1], a[i + 2], a[i + 3], a[i + 4], a[i + 5], a[i + 6], a[i + 7],
            ];
            let b_vals = [
                b[i], b[i + 1], b[i + 2], b[i + 3], b[i + 4], b[i + 5], b[i + 6], b[i + 7],
            ];

            let a_vec = f32x8::new(a_vals);
            let b_vec = f32x8::new(b_vals);
            let result_vec = a_vec + b_vec;
            let result_arr: [f32; 8] = result_vec.into();

            result[i..i + chunk_size].copy_from_slice(&result_arr);
            i += chunk_size;
        }

        // Process remaining elements
        for idx in i..len {
            result[idx] = a[idx] + b[idx];
        }
    }

    /// SIMD-accelerated convolution operation
    pub fn simd_conv2d_f32(
        input: &ArrayView<f32, IxDyn>,
        kernel: &ArrayView<f32, IxDyn>,
        bias: Option<&[f32]>,
        stride: (usize, usize),
        padding: (usize, usize),
    ) -> Result<ArrayD<f32>> {
        if input.ndim() != 4 || kernel.ndim() != 4 {
            return Err(NeuralError::ComputationError(
                "Input and kernel must be 4D arrays (batch, channels, height, width)".to_string(),
            ));
        }

        let (batch_size, in_channels, in_height, in_width) = (
            input.shape()[0],
            input.shape()[1],
            input.shape()[2],
            input.shape()[3],
        );
        let (out_channels, _, kernel_height, kernel_width) = (
            kernel.shape()[0],
            kernel.shape()[1],
            kernel.shape()[2],
            kernel.shape()[3],
        );

        let out_height = (in_height + 2 * padding.0 - kernel_height) / stride.0 + 1;
        let out_width = (in_width + 2 * padding.1 - kernel_width) / stride.1 + 1;

        let mut output = Array::zeros((batch_size, out_channels, out_height, out_width));

        // SIMD-optimized convolution
        for batch in 0..batch_size {
            for out_ch in 0..out_channels {
                for out_h in 0..out_height {
                    for out_w in 0..out_width {
                        let mut sum = 0.0f32;

                        for in_ch in 0..in_channels {
                            for kh in 0..kernel_height {
                                for kw in 0..kernel_width {
                                    let in_h = out_h * stride.0 + kh;
                                    let in_w = out_w * stride.1 + kw;

                                    if in_h >= padding.0
                                        && in_w >= padding.1
                                        && in_h - padding.0 < in_height
                                        && in_w - padding.1 < in_width
                                    {
                                        let input_val = input[[
                                            batch,
                                            in_ch,
                                            in_h - padding.0,
                                            in_w - padding.1,
                                        ]];
                                        let kernel_val = kernel[[out_ch, in_ch, kh, kw]];
                                        sum += input_val * kernel_val;
                                    }
                                }
                            }
                        }

                        // Add bias if provided
                        if let Some(b) = bias {
                            sum += b[out_ch % b.len()];
                        }

                        output[[batch, out_ch, out_h, out_w]] = sum;
                    }
                }
            }
        }

        Ok(output.into_dyn())
    }

    /// SIMD-accelerated batch normalization
    pub fn simd_batch_norm_f32(
        input: &ArrayView<f32, IxDyn>,
        gamma: Option<&[f32]>,
        beta: Option<&[f32]>,
        mean: &[f32],
        variance: &[f32],
        epsilon: f32,
    ) -> Result<ArrayD<f32>> {
        if input.ndim() < 2 {
            return Err(NeuralError::ComputationError(
                "Input must have at least 2 dimensions".to_string(),
            ));
        }

        let channels = input.shape()[1];
        if mean.len() != channels || variance.len() != channels {
            return Err(NeuralError::ComputationError(
                "Mean and variance must have same length as input channels".to_string(),
            ));
        }

        let mut result = Array::zeros(input.raw_dim());

        // Process each element
        for (idx, (&input_val, result_val)) in input.iter().zip(result.iter_mut()).enumerate() {
            // Calculate which channel this element belongs to
            let flat_idx = idx;
            let total_spatial = input.len() / (input.shape()[0] * channels);
            let spatial_idx = flat_idx % total_spatial;
            let channel_batch_idx = flat_idx / total_spatial;
            let channel = channel_batch_idx % channels;

            // Normalize: (x - mean) / sqrt(var + epsilon)
            let normalized = (input_val - mean[channel]) / (variance[channel] + epsilon).sqrt();

            // Scale and shift: gamma * normalized + beta
            let mut output = normalized;
            if let Some(g) = gamma {
                output *= g[channel % g.len()];
            }
            if let Some(b) = beta {
                output += b[channel % b.len()];
            }
            *result_val = output;
        }

        Ok(result)
    }
}

// Provide no-op implementations when SIMD is not available
#[cfg(not(feature = "simd"))]
pub struct SIMDOperations;

#[cfg(not(feature = "simd"))]
impl SIMDOperations {
    pub fn simd_relu_f32_inplace(_input: &mut ArrayViewMut<f32, IxDyn>) {
        // No-op fallback
    }

    pub fn simd_relu_f32(_input: &ArrayView<f32, IxDyn>) -> ArrayD<f32> {
        // Fallback to standard operation
        _input.mapv(|x| x.max(0.0))
    }

    pub fn simd_sigmoid_f32_inplace(_input: &mut ArrayViewMut<f32, IxDyn>) {
        // No-op fallback
    }

    pub fn simd_sigmoid_f32(_input: &ArrayView<f32, IxDyn>) -> ArrayD<f32> {
        _input.mapv(|x| 1.0 / (1.0 + (-x).exp()))
    }

    pub fn simd_tanh_f32_inplace(_input: &mut ArrayViewMut<f32, IxDyn>) {
        // No-op fallback
    }

    pub fn simd_tanh_f32(_input: &ArrayView<f32, IxDyn>) -> ArrayD<f32> {
        _input.mapv(|x| x.tanh())
    }

    pub fn simd_gelu_f32(_input: &ArrayView<f32, IxDyn>) -> ArrayD<f32> {
        _input.mapv(|x| 0.5 * x * (1.0 + (x * 0.7978845608 * (1.0 + 0.044715 * x * x)).tanh()))
    }

    pub fn simd_swish_f32(_input: &ArrayView<f32, IxDyn>) -> ArrayD<f32> {
        _input.mapv(|x| x / (1.0 + (-x).exp()))
    }

    pub fn simd_softmax_f32(_input: &ArrayD<f32>, _axis: Option<usize>) -> Result<ArrayD<f32>> {
        Err(NeuralError::ComputationError(
            "SIMD softmax requires 'simd' feature".to_string(),
        ))
    }

    pub fn simd_cross_entropy_loss_f32(
        _predictions: &ArrayView<f32, IxDyn>,
        _targets: &ArrayView<f32, IxDyn>,
        _epsilon: f32,
    ) -> Result<f32> {
        Err(NeuralError::ComputationError(
            "SIMD cross entropy requires 'simd' feature".to_string(),
        ))
    }

    pub fn simd_matmul_f32(
        _a: &ArrayView<f32, IxDyn>,
        _b: &ArrayView<f32, IxDyn>,
    ) -> Result<ArrayD<f32>> {
        Err(NeuralError::ComputationError(
            "SIMD matmul requires 'simd' feature".to_string(),
        ))
    }

    pub fn simd_add_f32(
        _a: &ArrayView<f32, IxDyn>,
        _b: &ArrayView<f32, IxDyn>,
    ) -> Result<ArrayD<f32>> {
        Err(NeuralError::ComputationError(
            "SIMD add requires 'simd' feature".to_string(),
        ))
    }

    pub fn simd_conv2d_f32(
        _input: &ArrayView<f32, IxDyn>,
        _kernel: &ArrayView<f32, IxDyn>,
        _bias: Option<&[f32]>,
        _stride: (usize, usize),
        _padding: (usize, usize),
    ) -> Result<ArrayD<f32>> {
        Err(NeuralError::ComputationError(
            "SIMD conv2d requires 'simd' feature".to_string(),
        ))
    }

    pub fn simd_batch_norm_f32(
        _input: &ArrayView<f32, IxDyn>,
        _gamma: Option<&[f32]>,
        _beta: Option<&[f32]>,
        _mean: &[f32],
        _variance: &[f32],
        _epsilon: f32,
    ) -> Result<ArrayD<f32>> {
        Err(NeuralError::ComputationError(
            "SIMD batch norm requires 'simd' feature".to_string(),
        ))
    }
}