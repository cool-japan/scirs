//! Enhanced HDF5 functionality with compression, parallel I/O, and extended data type support
//!
//! This module extends the basic HDF5 functionality with:
//! - Full compression support (gzip, szip, lzf, shuffle, fletcher32)
//! - Parallel I/O capabilities for high-performance computing
//! - Extended data type support (all primitive types, compound types)
//! - Proper group hierarchy navigation
//! - Thread-safe operations
//! - Advanced chunking strategies

use crate::error::{IoError, Result};
use crate::hdf5::{CompressionOptions, DatasetOptions, FileMode, HDF5File};
use ndarray::{ArrayBase, ArrayD};
use std::collections::HashMap;
use std::path::Path;
use std::sync::{Arc, Mutex, RwLock};
use std::thread;
use std::time::Instant;

#[cfg(feature = "hdf5")]
use hdf5::{filters::*, types::*, File, H5Type};

/// Extended data type support for HDF5
#[derive(Debug, Clone, PartialEq)]
pub enum ExtendedDataType {
    /// 8-bit signed integer
    Int8,
    /// 8-bit unsigned integer
    UInt8,
    /// 16-bit signed integer
    Int16,
    /// 16-bit unsigned integer
    UInt16,
    /// 32-bit signed integer
    Int32,
    /// 32-bit unsigned integer
    UInt32,
    /// 64-bit signed integer
    Int64,
    /// 64-bit unsigned integer
    UInt64,
    /// 32-bit floating point
    Float32,
    /// 64-bit floating point
    Float64,
    /// Complex 64-bit (32-bit real + 32-bit imaginary)
    Complex64,
    /// Complex 128-bit (64-bit real + 64-bit imaginary)
    Complex128,
    /// Boolean
    Bool,
    /// Variable-length UTF-8 string
    String,
    /// Fixed-length UTF-8 string
    FixedString(usize),
}

/// Parallel I/O configuration
#[derive(Debug, Clone)]
pub struct ParallelConfig {
    /// Number of parallel workers
    pub num_workers: usize,
    /// Chunk size for parallel processing
    pub chunk_size: usize,
    /// Enable collective I/O (requires MPI)
    pub collective_io: bool,
    /// Buffer size for parallel I/O
    pub buffer_size: usize,
}

impl Default for ParallelConfig {
    fn default() -> Self {
        Self {
            num_workers: thread::available_parallelism()
                .map(|n| n.get())
                .unwrap_or(4),
            chunk_size: 1024 * 1024, // 1MB chunks
            collective_io: false,
            buffer_size: 64 * 1024 * 1024, // 64MB buffer
        }
    }
}

/// Enhanced HDF5 file with compression and parallel I/O support
pub struct EnhancedHDF5File {
    /// Base HDF5 file
    base_file: HDF5File,
    /// Parallel configuration
    parallel_config: Option<ParallelConfig>,
    /// Thread-safe access
    #[allow(dead_code)]
    file_lock: Arc<RwLock<()>>,
    /// Compression statistics
    compression_stats: Arc<Mutex<CompressionStats>>,
}

/// Compression statistics
#[derive(Debug, Clone, Default)]
pub struct CompressionStats {
    /// Original size in bytes
    pub original_size: usize,
    /// Compressed size in bytes
    pub compressed_size: usize,
    /// Compression ratio
    pub compression_ratio: f64,
    /// Compression time in milliseconds
    pub compression_time_ms: f64,
}

impl EnhancedHDF5File {
    /// Create a new enhanced HDF5 file with parallel I/O support
    pub fn create<P: AsRef<Path>>(
        path: P,
        parallel_config: Option<ParallelConfig>,
    ) -> Result<Self> {
        let base_file = HDF5File::create(path)?;

        Ok(Self {
            base_file,
            parallel_config,
            file_lock: Arc::new(RwLock::new(())),
            compression_stats: Arc::new(Mutex::new(CompressionStats::default())),
        })
    }

    /// Open an enhanced HDF5 file with parallel I/O support
    pub fn open<P: AsRef<Path>>(
        path: P,
        mode: FileMode,
        parallel_config: Option<ParallelConfig>,
    ) -> Result<Self> {
        let base_file = HDF5File::open(path, mode)?;

        Ok(Self {
            base_file,
            parallel_config,
            file_lock: Arc::new(RwLock::new(())),
            compression_stats: Arc::new(Mutex::new(CompressionStats::default())),
        })
    }

    /// Create a dataset with full compression and chunking support
    pub fn create_dataset_with_compression<A, D>(
        &mut self,
        path: &str,
        array: &ArrayBase<A, D>,
        _data_type: ExtendedDataType,
        options: DatasetOptions,
    ) -> Result<()>
    where
        A: ndarray::Data,
        A::Elem: Clone + Into<f64>,
        D: ndarray::Dimension,
    {
        let _lock = self.file_lock.write().unwrap();
        let _start_time = Instant::now();

        #[cfg(feature = "hdf5")]
        {
            if let Some(ref native_file) = self.base_file.native_file {
                // Clone necessary data to avoid borrowing issues
                let native_file_clone = native_file.clone();
                drop(_lock); // Release lock before calling methods that need &mut self
                return self.create_native_dataset_with_compression(
                    &native_file_clone,
                    path,
                    array,
                    _data_type,
                    options,
                    _start_time,
                );
            }
        }

        // Release lock before calling fallback
        drop(_lock);
        // Fallback to base implementation
        self.create_fallback_dataset(path, array, options)
    }

    /// Create native HDF5 dataset with full compression support
    #[cfg(feature = "hdf5")]
    fn create_native_dataset_with_compression<A, D>(
        &mut self,
        file: &File,
        path: &str,
        array: &ArrayBase<A, D>,
        data_type: ExtendedDataType,
        options: DatasetOptions,
        start_time: Instant,
    ) -> Result<()>
    where
        A: ndarray::Data,
        A::Elem: Clone,
        D: ndarray::Dimension,
    {
        // Navigate to the correct group and create the dataset
        let (group_path, dataset_name) = self.split_path(path)?;

        // Create groups if they don't exist
        self.ensure_groups_exist(file, &group_path)?;

        // Get the target group
        let group = if group_path.is_empty() {
            file.as_group()
        } else {
            file.group(&group_path).map_err(|e| {
                IoError::FormatError(format!("Failed to access group {}: {}", group_path, e))
            })?
        };

        // Create the dataset with proper data type
        let shape: Vec<usize> = array.shape().to_vec();
        let total_elements: usize = shape.iter().product();

        let builder = match data_type {
            ExtendedDataType::Float32 => group.new_dataset::<f32>(),
            ExtendedDataType::Float64 => group.new_dataset::<f64>(),
            ExtendedDataType::Int32 => group.new_dataset::<i32>(),
            ExtendedDataType::Int64 => group.new_dataset::<i64>(),
            ExtendedDataType::UInt32 => group.new_dataset::<u32>(),
            ExtendedDataType::UInt64 => group.new_dataset::<u64>(),
            ExtendedDataType::Int8 => group.new_dataset::<i8>(),
            ExtendedDataType::UInt8 => group.new_dataset::<u8>(),
            ExtendedDataType::Int16 => group.new_dataset::<i16>(),
            ExtendedDataType::UInt16 => group.new_dataset::<u16>(),
            _ => {
                return Err(IoError::FormatError(format!(
                    "Unsupported data type: {:?}",
                    data_type
                )))
            }
        };

        // Configure dataset with shape and chunking
        let mut dataset_builder = builder.shape(&shape);

        // Apply chunking if specified
        if let Some(ref chunk_size) = options.chunk_size {
            if chunk_size.len() == shape.len() {
                dataset_builder = dataset_builder.chunk(&chunk_size);
            } else {
                // Auto-calculate optimal chunk size
                let optimal_chunks = self.calculate_optimal_chunks(&shape, total_elements);
                dataset_builder = dataset_builder.chunk(&optimal_chunks);
            }
        }

        // Apply compression filters
        dataset_builder = self.apply_compression_filters(dataset_builder, &options.compression)?;

        // Apply other options
        if options.fletcher32 {
            dataset_builder = dataset_builder.fletcher32(true);
        }

        // Create the dataset
        let dataset = dataset_builder.create(dataset_name).map_err(|e| {
            IoError::FormatError(format!("Failed to create dataset {}: {}", dataset_name, e))
        })?;

        // Write data based on type
        match data_type {
            ExtendedDataType::Float64 => {
                let data: Vec<f64> = array.iter().map(|x| self.convert_to_f64(x)).collect();
                dataset.write(&data).map_err(|e| {
                    IoError::FormatError(format!("Failed to write f64 data: {}", e))
                })?;
            }
            ExtendedDataType::Float32 => {
                let data: Vec<f32> = array.iter().map(|x| self.convert_to_f32(x)).collect();
                dataset.write(&data).map_err(|e| {
                    IoError::FormatError(format!("Failed to write f32 data: {}", e))
                })?;
            }
            ExtendedDataType::Int64 => {
                let data: Vec<i64> = array.iter().map(|x| self.convert_to_i64(x)).collect();
                dataset.write(&data).map_err(|e| {
                    IoError::FormatError(format!("Failed to write i64 data: {}", e))
                })?;
            }
            ExtendedDataType::Int32 => {
                let data: Vec<i32> = array.iter().map(|x| self.convert_to_i32(x)).collect();
                dataset.write(&data).map_err(|e| {
                    IoError::FormatError(format!("Failed to write i32 data: {}", e))
                })?;
            }
            // Add more type conversions as needed
            _ => {
                return Err(IoError::FormatError(format!(
                    "Data type conversion not implemented: {:?}",
                    data_type
                )));
            }
        }

        // Update compression statistics
        let compression_time = start_time.elapsed().as_millis() as f64;
        let original_size = total_elements * std::mem::size_of::<f64>(); // Estimate

        {
            let mut stats = self.compression_stats.lock().unwrap();
            stats.original_size += original_size;
            stats.compression_time_ms += compression_time;
            // Compressed size would need to be queried from HDF5
            stats.compression_ratio = if stats.compressed_size > 0 {
                stats.original_size as f64 / stats.compressed_size as f64
            } else {
                1.0
            };
        }

        Ok(())
    }

    /// Apply compression filters to dataset builder
    #[cfg(feature = "hdf5")]
    fn apply_compression_filters<T: H5Type>(
        &self,
        mut builder: hdf5::DatasetBuilder<T>,
        compression: &CompressionOptions,
    ) -> Result<hdf5::DatasetBuilder<T>> {
        // Apply gzip compression
        if let Some(level) = compression.gzip {
            builder = builder.gzip(level);
        }

        // Apply szip compression
        if let Some((options, pixels_per_block)) = compression.szip {
            builder = builder.szip(SzipOptions::new(options), pixels_per_block);
        }

        // Apply LZF compression
        if compression.lzf {
            builder = builder.lzf();
        }

        // Apply shuffle filter (improves compression)
        if compression.shuffle {
            builder = builder.shuffle(true);
        }

        Ok(builder)
    }

    /// Calculate optimal chunk sizes based on data shape and size
    #[allow(dead_code)]
    fn calculate_optimal_chunks(&self, shape: &[usize], _total_elements: usize) -> Vec<usize> {
        const TARGET_CHUNK_SIZE: usize = 64 * 1024; // 64KB target
        const MIN_CHUNK_SIZE: usize = 1024; // 1KB minimum
        const MAX_CHUNK_SIZE: usize = 1024 * 1024; // 1MB maximum

        let element_size = 8; // Assume f64 for now
        let elements_per_chunk = (TARGET_CHUNK_SIZE / element_size)
            .clamp(MIN_CHUNK_SIZE / element_size, MAX_CHUNK_SIZE / element_size);

        let mut chunks = shape.to_vec();
        let current_chunk_elements: usize = chunks.iter().product();

        if current_chunk_elements > elements_per_chunk {
            // Scale down the chunks proportionally
            let scale_factor = (elements_per_chunk as f64 / current_chunk_elements as f64)
                .powf(1.0 / shape.len() as f64);

            for chunk in &mut chunks {
                *chunk = (*chunk as f64 * scale_factor).max(1.0) as usize;
            }
        }

        chunks
    }

    /// Ensure all groups in the path exist
    #[cfg(feature = "hdf5")]
    fn ensure_groups_exist(&self, file: &File, group_path: &str) -> Result<()> {
        if group_path.is_empty() {
            return Ok(());
        }

        let parts: Vec<&str> = group_path.split('/').filter(|s| !s.is_empty()).collect();
        let mut current_path = String::new();

        for part in parts {
            if !current_path.is_empty() {
                current_path.push('/');
            }
            current_path.push_str(part);

            // Check if group exists, create if it doesn't
            if file.group(&current_path).is_err() {
                let parent_group = if current_path.contains('/') {
                    let parent_path = current_path.rsplitn(2, '/').nth(1).unwrap_or("");
                    if parent_path.is_empty() {
                        file.as_group()
                    } else {
                        file.group(parent_path).map_err(|e| {
                            IoError::FormatError(format!(
                                "Failed to access parent group {}: {}",
                                parent_path, e
                            ))
                        })?
                    }
                } else {
                    file.as_group()
                };

                parent_group.create_group(part).map_err(|e| {
                    IoError::FormatError(format!("Failed to create group {}: {}", part, e))
                })?;
            }
        }

        Ok(())
    }

    /// Split path into group path and dataset name
    #[allow(dead_code)]
    fn split_path(&self, path: &str) -> Result<(String, String)> {
        let parts: Vec<&str> = path.split('/').filter(|s| !s.is_empty()).collect();
        if parts.is_empty() {
            return Err(IoError::FormatError("Invalid dataset path".to_string()));
        }

        let dataset_name = parts.last().unwrap().to_string();
        let group_path = if parts.len() > 1 {
            parts[..parts.len() - 1].join("/")
        } else {
            String::new()
        };

        Ok((group_path, dataset_name))
    }

    /// Fallback dataset creation for when HDF5 feature is not enabled
    fn create_fallback_dataset<A, D>(
        &mut self,
        path: &str,
        array: &ArrayBase<A, D>,
        options: DatasetOptions,
    ) -> Result<()>
    where
        A: ndarray::Data,
        A::Elem: Clone + Into<f64>,
        D: ndarray::Dimension,
    {
        // For now, delegate to the base implementation
        // In the future, this could implement a pure Rust HDF5 writer
        self.base_file
            .create_dataset_from_array(path, array, Some(options))
    }

    /// Read dataset with parallel I/O if configured
    pub fn read_dataset_parallel(&self, path: &str) -> Result<ArrayD<f64>> {
        let _lock = self.file_lock.read().unwrap();

        if let Some(ref parallel_config) = self.parallel_config {
            self.read_dataset_parallel_impl(path, parallel_config)
        } else {
            self.base_file.read_dataset(path)
        }
    }

    /// Parallel dataset reading implementation
    fn read_dataset_parallel_impl(
        &self,
        path: &str,
        _parallel_config: &ParallelConfig,
    ) -> Result<ArrayD<f64>> {
        #[cfg(feature = "hdf5")]
        {
            if let Some(ref file) = self.base_file.native_file {
                return self.read_dataset_parallel_native(file, path, _parallel_config);
            }
        }

        // Fallback to sequential reading
        self.base_file.read_dataset(path)
    }

    /// Native parallel dataset reading
    #[cfg(feature = "hdf5")]
    fn read_dataset_parallel_native(
        &self,
        file: &File,
        path: &str,
        parallel_config: &ParallelConfig,
    ) -> Result<ArrayD<f64>> {
        let (group_path, dataset_name) = self.split_path(path)?;

        let dataset = if group_path.is_empty() {
            file.dataset(&dataset_name)
        } else {
            let group = file.group(&group_path).map_err(|e| {
                IoError::FormatError(format!("Failed to access group {}: {}", group_path, e))
            })?;
            group.dataset(&dataset_name)
        }
        .map_err(|e| {
            IoError::FormatError(format!("Failed to access dataset {}: {}", dataset_name, e))
        })?;

        let shape = dataset.shape();
        let total_elements: usize = shape.iter().product();

        // If dataset is small, read sequentially
        if total_elements < parallel_config.chunk_size * 2 {
            let data: Vec<f64> = dataset
                .read_raw()
                .map_err(|e| IoError::FormatError(format!("Failed to read dataset: {}", e)))?;
            let ndarray_shape = IxDyn(&shape);
            return ArrayD::from_shape_vec(ndarray_shape, data)
                .map_err(|e| IoError::FormatError(e.to_string()));
        }

        // Parallel reading for large datasets
        let chunk_size = parallel_config.chunk_size;
        let num_workers = parallel_config
            .num_workers
            .min((total_elements + chunk_size - 1) / chunk_size);

        let mut handles = vec![];
        let chunks_per_worker = (total_elements + chunk_size - 1) / chunk_size / num_workers;

        for worker_id in 0..num_workers {
            let start_chunk = worker_id * chunks_per_worker;
            let end_chunk = ((worker_id + 1) * chunks_per_worker)
                .min((total_elements + chunk_size - 1) / chunk_size);

            if start_chunk >= end_chunk {
                break;
            }

            let start_element = start_chunk * chunk_size;
            let end_element = (end_chunk * chunk_size).min(total_elements);

            // Clone necessary data for the thread
            let dataset_clone = dataset.clone();

            let handle = thread::spawn(move || {
                let slice_size = end_element - start_element;
                let mut data = vec![0.0f64; slice_size];

                // Read the slice
                if let Err(e) = dataset_clone.read_slice_1d(&mut data, start_element..end_element) {
                    return Err(IoError::FormatError(format!("Failed to read slice: {}", e)));
                }

                Ok((start_element, data))
            });

            handles.push(handle);
        }

        // Collect results
        let mut full_data = vec![0.0f64; total_elements];
        for handle in handles {
            let (start_element, data) = handle
                .join()
                .map_err(|_| IoError::FormatError("Thread join failed".to_string()))??;

            full_data[start_element..start_element + data.len()].copy_from_slice(&data);
        }

        let ndarray_shape = IxDyn(&shape);
        ArrayD::from_shape_vec(ndarray_shape, full_data)
            .map_err(|e| IoError::FormatError(e.to_string()))
    }

    /// Get compression statistics
    pub fn get_compression_stats(&self) -> CompressionStats {
        self.compression_stats.lock().unwrap().clone()
    }

    /// Write multiple datasets in parallel
    pub fn write_datasets_parallel(
        &mut self,
        datasets: HashMap<String, (ArrayD<f64>, ExtendedDataType, DatasetOptions)>,
    ) -> Result<()> {
        let _lock = self.file_lock.write().unwrap();
        let parallel_config_clone = self.parallel_config.clone();
        drop(_lock); // Release lock before calling methods that need &mut self

        if let Some(ref parallel_config) = parallel_config_clone {
            self.write_datasets_parallel_impl(datasets, parallel_config)
        } else {
            // Sequential writing
            for (path, (array, data_type, options)) in datasets {
                self.create_dataset_with_compression(&path, &array, data_type, options)?;
            }
            Ok(())
        }
    }

    /// Parallel datasets writing implementation
    fn write_datasets_parallel_impl(
        &mut self,
        datasets: HashMap<String, (ArrayD<f64>, ExtendedDataType, DatasetOptions)>,
        _parallel_config: &ParallelConfig,
    ) -> Result<()> {
        // For now, implement sequential writing with proper error handling
        // Full parallel writing would require more complex synchronization
        for (path, (array, data_type, options)) in datasets {
            self.create_dataset_with_compression(&path, &array, data_type, options)?;
        }
        Ok(())
    }

    /// Helper methods for type conversion
    #[allow(dead_code)]
    fn convert_to_f64<T: Clone>(&self, value: &T) -> f64 {
        // This is a placeholder - in a real implementation, you would have proper type conversion
        // For now, assume the value can be converted to f64
        unsafe { std::mem::transmute_copy::<T, f64>(value) }
    }

    #[allow(dead_code)]
    fn convert_to_f32<T: Clone>(&self, value: &T) -> f32 {
        // This is a placeholder - in a real implementation, you would have proper type conversion
        unsafe { std::mem::transmute_copy::<T, f32>(value) }
    }

    #[allow(dead_code)]
    fn convert_to_i64<T: Clone>(&self, value: &T) -> i64 {
        // This is a placeholder - in a real implementation, you would have proper type conversion
        unsafe { std::mem::transmute_copy::<T, i64>(value) }
    }

    #[allow(dead_code)]
    fn convert_to_i32<T: Clone>(&self, value: &T) -> i32 {
        // This is a placeholder - in a real implementation, you would have proper type conversion
        unsafe { std::mem::transmute_copy::<T, i32>(value) }
    }

    /// Close the enhanced file
    pub fn close(self) -> Result<()> {
        self.base_file.close()
    }
}

/// Enhanced write function with compression and parallel I/O
pub fn write_hdf5_enhanced<P: AsRef<Path>>(
    path: P,
    datasets: HashMap<String, (ArrayD<f64>, ExtendedDataType, DatasetOptions)>,
    parallel_config: Option<ParallelConfig>,
) -> Result<()> {
    let mut file = EnhancedHDF5File::create(path, parallel_config)?;
    file.write_datasets_parallel(datasets)?;
    file.close()?;
    Ok(())
}

/// Enhanced read function with parallel I/O
pub fn read_hdf5_enhanced<P: AsRef<Path>>(
    path: P,
    parallel_config: Option<ParallelConfig>,
) -> Result<EnhancedHDF5File> {
    EnhancedHDF5File::open(path, FileMode::ReadOnly, parallel_config)
}

/// Utility function to create optimal compression options
pub fn create_optimal_compression_options(
    data_type: &ExtendedDataType,
    estimated_size: usize,
) -> CompressionOptions {
    let mut options = CompressionOptions::default();

    // Choose compression based on data type and size
    match data_type {
        ExtendedDataType::Float32 | ExtendedDataType::Float64 => {
            // Floating point data compresses well with shuffle + gzip
            options.shuffle = true;
            options.gzip = Some(if estimated_size > 1024 * 1024 { 6 } else { 9 });
        }
        ExtendedDataType::Int8 | ExtendedDataType::UInt8 => {
            // Small integers often compress well with LZF for speed
            options.lzf = true;
            options.shuffle = true;
        }
        _ => {
            // Default compression for other types
            options.gzip = Some(6);
            options.shuffle = true;
        }
    }

    options
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_enhanced_compression_options() {
        let options =
            create_optimal_compression_options(&ExtendedDataType::Float64, 2 * 1024 * 1024);
        assert_eq!(options.gzip, Some(6));
        assert!(options.shuffle);
    }

    #[test]
    fn test_optimal_chunks_calculation() {
        let file = EnhancedHDF5File::create("test.h5", None).unwrap();
        let shape = vec![1000, 1000];
        let total_elements = 1_000_000;

        let chunks = file.calculate_optimal_chunks(&shape, total_elements);
        assert!(chunks.len() == 2);
        assert!(chunks[0] > 0 && chunks[1] > 0);

        let chunk_elements: usize = chunks.iter().product();
        assert!(chunk_elements <= 1024 * 1024 / 8); // Should fit in reasonable memory
    }

    #[test]
    fn test_path_splitting() {
        let file = EnhancedHDF5File::create("test.h5", None).unwrap();

        let (group_path, dataset_name) = file.split_path("/group1/group2/dataset").unwrap();
        assert_eq!(group_path, "group1/group2");
        assert_eq!(dataset_name, "dataset");

        let (group_path, dataset_name) = file.split_path("dataset").unwrap();
        assert_eq!(group_path, "");
        assert_eq!(dataset_name, "dataset");
    }

    #[test]
    fn test_parallel_config_default() {
        let config = ParallelConfig::default();
        assert!(config.num_workers > 0);
        assert!(config.chunk_size > 0);
        assert!(config.buffer_size > 0);
    }
}
