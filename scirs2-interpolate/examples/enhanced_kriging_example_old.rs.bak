/// Enhanced Kriging Demonstration Example
///
/// This example demonstrates the advanced kriging capabilities in scirs2-interpolate,
/// including:
///
/// 1. Anisotropic Kriging:
///    - Direction-dependent spatial correlation
///    - Different correlation ranges in different directions
///    - Rotated correlation structure
///
/// 2. Universal Kriging:
///    - Deterministic trend functions (constant, linear, quadratic)
///    - Parameter estimation for trend coefficients
///    - Combined trend + residual modeling
///
/// 3. Bayesian Kriging:
///    - Parameter uncertainty through priors and posteriors
///    - Comprehensive prediction uncertainty
///    - Credible intervals for predictions
///
/// 4. Model Selection:
///    - Comparing models using marginal likelihood
///    - Cross-validation for performance assessment
///    - Optimal covariance function selection
///
/// Each example builds on key geostatistical concepts and demonstrates
/// how to use the various kriging implementations for spatial data modeling.
use ndarray::{s, array, Array1, Array2, Axis, ArrayView1};
use num_traits::{Float, FromPrimitive};
use std::fmt::Debug;
use scirs2_interpolate::advanced::enhanced_kriging::{
    AnisotropicCovariance, BayesianKrigingBuilder, EnhancedKriging,
    EnhancedKrigingBuilder, TrendFunction,
};
use scirs2_interpolate::advanced::kriging::CovarianceFunction;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("Enhanced Kriging Example");
    println!("========================\n");

    // Generate sample data with anisotropic behavior
    let (points, values) = generate_anisotropic_data();
    println!(
        "Generated {} sample points with anisotropic covariance",
        points.shape()[0]
    );

    // Example 1: Basic Kriging with Anisotropic Covariance
    println!("\n1. Anisotropic Kriging Example");
    println!("------------------------------");
    anisotropic_kriging_example(&points, &values)?;

    // Example 2: Universal Kriging with trend functions
    println!("\n2. Universal Kriging Example");
    println!("----------------------------");
    universal_kriging_example(&points, &values)?;

    // Example 3: Bayesian Kriging with uncertainty quantification
    println!("\n3. Bayesian Kriging Example");
    println!("---------------------------");
    bayesian_kriging_example(&points, &values)?;

    // Example 4: Model comparison using log marginal likelihood
    println!("\n4. Model Comparison Example");
    println!("--------------------------");
    model_comparison_example(&points, &values)?;

    Ok(())
}

/// Generate synthetic data with anisotropic covariance structure
fn generate_anisotropic_data() -> (Array2<f64>, Array1<f64>) {
    let n_samples = 100;
    use scirs2_core::random::Random;
    let mut rng = Random::default();

    // Create a grid of points
    let mut points = Array2::zeros((n_samples, 2));
    let mut values = Array1::zeros(n_samples);

    // Generate random points in the domain [0, 10] x [0, 10]
    for i in 0..n_samples {
        points[[i, 0]] = rng.uniform(0.0, 10.0);
        points[[i, 1]] = rng.uniform(0.0, 10.0);
    }

    // Generate values with anisotropic trend
    // Function with stronger variation in x direction than y direction
    // f(x,y) = sin(0.5*x) + 0.2*sin(2*y) + 0.1*x*y
    for i in 0..n_samples {
        let x = points[[i, 0]];
        let y = points[[i, 1]];
        values[i] = (0.5 * x).sin() + 0.2 * (2.0 * y).sin() + 0.1 * x * y;

        // Add some random noise
        values[i] += rng.normal(0.0, 0.05);
    }

    (points, values)
}

/// Generate a grid of points for prediction
fn generate_prediction_grid(n_grid: usize) -> Array2<f64> {
    let grid_size = n_grid * n_grid;
    let mut grid_points = Array2::zeros((grid_size, 2));

    let mut idx = 0;
    for i in 0..n_grid {
        let x = 10.0 * (i as f64) / ((n_grid - 1) as f64);
        for j in 0..n_grid {
            let y = 10.0 * (j as f64) / ((n_grid - 1) as f64);
            grid_points[[idx, 0]] = x;
            grid_points[[idx, 1]] = y;
            idx += 1;
        }
    }

    grid_points
}

/// Example demonstrating anisotropic kriging
///
/// Anisotropic kriging allows for direction-dependent spatial correlation,
/// which is important for many physical processes:
///
/// * Geological formations often have different correlation lengths horizontally vs. vertically
/// * Wind-driven processes may have stronger correlation in the prevailing wind direction
/// * Hydrological processes follow terrain and flow directions
///
/// This example demonstrates:
/// 1. Creating an anisotropic covariance specification with different length scales
/// 2. Adding rotation to the correlation structure
/// 3. Measuring how predictions vary differently in each direction
fn anisotropic_kriging_example(
    points: &Array2<f64>,
    values: &Array1<f64>,
) -> Result<(), Box<dyn std::error::Error>> {
    // Skip this example as EnhancedKrigingBuilder is not fully implemented
    println!("  EnhancedKrigingBuilder is not fully implemented in this version");
    return Ok(());

    // Create an anisotropic covariance function
    // Define different length scales for x and y directions
    let anisotropic_cov = AnisotropicCovariance::new(
        CovarianceFunction::Matern52,
        vec![2.0, 0.5],                         // Longer correlation range in x than y
        1.0,                                    // Variance
        0.1,                                    // Nugget
        Some(vec![std::f64::consts::PI / 6.0]), // Rotate by 30 degrees
    );

    // Create enhanced kriging model with anisotropic covariance
    let kriging = EnhancedKrigingBuilder::new()
        .points(points.clone())
        .values(values.clone())
        .anisotropic_covariance(anisotropic_cov)
        .build()?;

    // Make predictions at a few test points
    let test_points = Array2::from_shape_vec((3, 2), vec![1.5, 1.5, 5.0, 5.0, 8.0, 2.0])?;

    let predictions = kriging.predict(&test_points)?;
    let variances = kriging.predict_variance(&test_points)?;

    // Print predictions with confidence intervals
    println!("Predictions at test points:");
    println!("Point\t\tPrediction\t95% Confidence Interval");
    for i in 0..test_points.shape()[0] {
        let point = test_points.slice(s![i, ..]);
        let pred = predictions[i];
        let std_dev = variances[i].sqrt();

        println!(
            "({:.1}, {:.1})\t{:.4}\t\t[{:.4}, {:.4}]",
            point[0],
            point[1],
            pred,
            pred - 1.96 * std_dev,
            pred + 1.96 * std_dev
        );
    }

    // Demonstrate the effect of anisotropy
    println!("\nDemonstrating anisotropy effect:");
    let point = array![5.0, 5.0];
    let dx_point = array![6.0, 5.0]; // Move 1 unit in x direction
    let dy_point = array![5.0, 6.0]; // Move 1 unit in y direction

    let base_pred = kriging.predict(&point.into_shape((1, 2))?)?[0];
    let dx_pred = kriging.predict(&dx_point.into_shape((1, 2))?)?[0];
    let dy_pred = kriging.predict(&dy_point.into_shape((1, 2))?)?[0];

    println!("Base point (5.0, 5.0): {:.4}", base_pred);
    println!(
        "Move 1 unit in x direction: {:.4} (change: {:.4})",
        dx_pred,
        dx_pred - base_pred
    );
    println!(
        "Move 1 unit in y direction: {:.4} (change: {:.4})",
        dy_pred,
        dy_pred - base_pred
    );

    Ok(())
}

/// Example demonstrating Universal Kriging with trend functions
///
/// Universal Kriging extends ordinary kriging by adding a deterministic trend component
/// to model non-stationary processes. This is essential for:
///
/// * Data with systematic spatial trends (e.g., temperature decreasing with elevation)
/// * Variables influenced by known physical processes
/// * Cases where the assumption of constant mean is inappropriate
///
/// The model takes the form: Z(x) = m(x) + ε(x) where:
/// * m(x) is the deterministic trend (e.g., linear, quadratic)
/// * ε(x) is the spatially correlated residual process
///
/// This example demonstrates:
/// 1. Comparing different trend functions (constant, linear, quadratic)
/// 2. Extracting and interpreting trend coefficients
/// 3. Visualizing how the trend component affects predictions
fn universal_kriging_example(
    points: &Array2<f64>,
    values: &Array1<f64>,
) -> Result<(), Box<dyn std::error::Error>> {
    // Skip this example as EnhancedKrigingBuilder is not fully implemented
    println!("  EnhancedKrigingBuilder is not fully implemented in this version");
    return Ok(());
    
    // Create kriging models with different trend functions
    let standard_kriging = EnhancedKrigingBuilder::new()
        .points(points.clone())
        .values(values.clone())
        .trend_function(TrendFunction::Constant)
        .covariance_function(CovarianceFunction::Gaussian)
        .build()?;

    let linear_trend_kriging = EnhancedKrigingBuilder::new()
        .points(points.clone())
        .values(values.clone())
        .trend_function(TrendFunction::Linear)
        .covariance_function(CovarianceFunction::Gaussian)
        .build()?;

    let quadratic_trend_kriging = EnhancedKrigingBuilder::new()
        .points(points.clone())
        .values(values.clone())
        .trend_function(TrendFunction::Quadratic)
        .covariance_function(CovarianceFunction::Gaussian)
        .build()?;

    // Generate a line of test points
    let mut test_line = Array2::zeros((11, 2));
    for i in 0..11 {
        let x = i as f64;
        test_line[[i, 0]] = x;
        test_line[[i, 1]] = 5.0; // Fixed y coordinate
    }

    // Make predictions with each model
    let std_preds = standard_kriging.predict(&test_line)?;
    let linear_preds = linear_trend_kriging.predict(&test_line)?;
    let quad_preds = quadratic_trend_kriging.predict(&test_line)?;

    // Print trend comparison
    println!("Comparing trend functions along x-axis (y=5):");
    println!("   x   | Constant | Linear  | Quadratic");
    println!("-------+----------+---------+----------");
    for i in 0..test_line.shape()[0] {
        println!(
            " {:.1}   |  {:.4}  |  {:.4}  |  {:.4}",
            test_line[[i, 0]],
            std_preds[i],
            linear_preds[i],
            quad_preds[i]
        );
    }

    // Extract and print the trend coefficients
    if let Some(trend_coeffs) = linear_trend_kriging.trend_coefficients() {
        println!("\nLinear trend coefficients:");
        println!("Intercept: {:.4}", trend_coeffs[0]);
        println!("X coefficient: {:.4}", trend_coeffs[1]);
        println!("Y coefficient: {:.4}", trend_coeffs[2]);
    }

    if let Some(trend_coeffs) = quadratic_trend_kriging.trend_coefficients() {
        println!("\nQuadratic trend coefficients:");
        println!("Intercept: {:.4}", trend_coeffs[0]);
        println!("X coefficient: {:.4}", trend_coeffs[1]);
        println!("Y coefficient: {:.4}", trend_coeffs[2]);
        println!("X² coefficient: {:.4}", trend_coeffs[3]);
        println!("XY coefficient: {:.4}", trend_coeffs[4]);
        println!("Y² coefficient: {:.4}", trend_coeffs[5]);
    }

    Ok(())
}

/// Example demonstrating Bayesian Kriging with uncertainty quantification
///
/// Bayesian Kriging treats model parameters (length scales, variance, etc.) as
/// random variables with prior distributions, allowing for:
///
/// * Full uncertainty quantification, including parameter uncertainty
/// * More reliable prediction intervals
/// * Integration of prior knowledge about parameters
/// * Model comparison through marginal likelihood
///
/// The Bayesian approach gives us:
/// * Posterior distributions for all parameters
/// * Full predictive distributions, not just means and variances
/// * Credible intervals that account for all sources of uncertainty
///
/// This example demonstrates:
/// 1. Setting up prior distributions for model parameters
/// 2. Making predictions with full uncertainty quantification
/// 3. Analyzing parameter posterior distributions
/// 4. Computing credible intervals for predictions
fn bayesian_kriging_example(
    points: &Array2<f64>,
    values: &Array1<f64>,
) -> Result<(), Box<dyn std::error::Error>> {
    // Skip this example as BayesianKrigingBuilder is not fully implemented
    println!("  BayesianKrigingBuilder is not fully implemented in this version");
    return Ok(());
    
    // Create a Bayesian kriging model
    let bayesian_kriging = BayesianKrigingBuilder::new()
        .points(points.clone())
        .values(values.clone())
        .covariance_function(CovarianceFunction::Matern52)
        .length_scale_prior(0.5, 5.0) // Prior for length scale: between 0.5 and 5.0
        .variance_prior(0.1, 2.0)     // Prior for variance: between 0.1 and 2.0
        .nugget_prior(0.01, 0.2)      // Prior for nugget: between 0.01 and 0.2
        .n_samples(1000)              // Number of posterior samples
        .optimize_parameters(true)    // Optimize parameters before sampling
        .build()?;

    // Select test points for prediction
    let test_points = Array2::from_shape_vec((3, 2), vec![1.5, 1.5, 5.0, 5.0, 8.0, 2.0])?;

    // Get Bayesian predictions with uncertainty
    let bayes_results = bayesian_kriging.predict_with_uncertainty(&test_points)?;

    // Print Bayesian prediction results
    println!("Bayesian predictions with uncertainty quantification:");
    println!("Point\t\tMean\t\t95% Credible Interval\tSD");
    for i in 0..test_points.shape()[0] {
        let point = test_points.slice(s![i, ..]);
        let result = &bayes_results[i];

        println!(
            "({:.1}, {:.1})\t{:.4}\t\t[{:.4}, {:.4}]\t{:.4}",
            point[0],
            point[1],
            result.mean,
            result.quantiles[0], // 2.5% quantile
            result.quantiles[4], // 97.5% quantile
            result.std_dev
        );
    }

    // Display parameter distributions
    println!("\nParameter posterior distributions:");

    // Length scale parameters
    if let Some(length_scales) = bayesian_kriging.parameter_distribution("length_scale") {
        let length_scale_mean = length_scales.mean();
        let length_scale_std = length_scales.std(0.0);

        println!(
            "Length scale: mean = {:.4}, std = {:.4}",
            length_scale_mean, length_scale_std
        );
        println!("Length scale quantiles:");
        print_quantiles(&length_scales.view());
    }

    // Variance parameters
    if let Some(variances) = bayesian_kriging.parameter_distribution("sigma_sq") {
        let variance_mean = variances.mean();
        let variance_std = variances.std(0.0);

        println!(
            "Variance: mean = {:.4}, std = {:.4}",
            variance_mean, variance_std
        );
        println!("Variance quantiles:");
        print_quantiles(&variances.view());
    }

    // Nugget parameters
    if let Some(nuggets) = bayesian_kriging.parameter_distribution("nugget") {
        let nugget_mean = nuggets.mean();
        let nugget_std = nuggets.std(0.0);

        println!("Nugget: mean = {:.4}, std = {:.4}", nugget_mean, nugget_std);
        println!("Nugget quantiles:");
        print_quantiles(&nuggets.view());
    }

    Ok(())
}

/// Example demonstrating model comparison and selection methods
///
/// Selecting the optimal kriging model involves comparing different:
/// * Covariance functions (Gaussian, Matérn, exponential, etc.)
/// * Trend functions (constant, linear, quadratic)
/// * Parameter settings (optimized vs. fixed)
///
/// This example demonstrates two key approaches to model comparison:
///
/// 1. Marginal Likelihood (ML):
///    * Balances model fit and complexity
///    * Higher values indicate better models
///    * Accounts for parameter uncertainty
///    * Particularly useful for Bayesian models
///
/// 2. Cross-Validation (CV):
///    * Measures predictive performance on held-out data
///    * Less sensitive to model misspecification
///    * More computationally intensive
///    * Closer to real-world prediction scenarios
///
/// Together, these methods provide a robust framework for selecting
/// the most appropriate kriging model for a given dataset.
fn model_comparison_example(
    points: &Array2<f64>,
    values: &Array1<f64>,
) -> Result<(), Box<dyn std::error::Error>> {
    // Skip this example as model comparison is not fully implemented
    println!("  Model comparison is not fully implemented in this version");
    return Ok(());
    
    // Create kriging models with different covariance functions
    let covariance_functions = vec![
        CovarianceFunction::Gaussian,
        CovarianceFunction::Exponential,
        CovarianceFunction::Matern32,
        CovarianceFunction::Matern52,
        CovarianceFunction::RationalQuadratic,
    ];

    println!("Comparing kriging models with different covariance functions:");
    println!("Covariance Function\tLog Marginal Likelihood\tOptimal Parameters");

    for &cov_fn in &covariance_functions {
        // Build model with parameter optimization
        let kriging = EnhancedKrigingBuilder::new()
            .points(points.clone())
            .values(values.clone())
            .covariance_function(cov_fn)
            .optimize_parameters(true)
            .build()?;

        // Get log marginal likelihood
        let log_ml = kriging.log_marginal_likelihood();

        // Get optimized parameters
        let params = kriging.parameters();

        println!(
            "{:20}\t{:.4}\t\t{:?}",
            format!("{:?}", cov_fn),
            log_ml,
            params
        );
    }

    // Compare different trend functions with best covariance
    let trend_functions = vec![
        TrendFunction::Constant,
        TrendFunction::Linear,
        TrendFunction::Quadratic,
    ];

    println!("\nComparing kriging models with different trend functions:");
    println!("Trend Function\t\tLog Marginal Likelihood");

    for &trend_fn in &trend_functions {
        // Build model with parameter optimization
        let kriging = EnhancedKrigingBuilder::new()
            .points(points.clone())
            .values(values.clone())
            .covariance_function(CovarianceFunction::Matern52) // Use Matern 5/2 for all
            .trend_function(trend_fn)
            .optimize_parameters(true)
            .build()?;

        // Get log marginal likelihood
        let log_ml = kriging.log_marginal_likelihood();

        println!("{:20}\t{:.4}", format!("{:?}", trend_fn), log_ml);
    }

    // Cross-validation comparison
    println!("\nCross-validation comparison (5-fold):");
    println!("Model\t\t\tMean Squared Error");

    // Best covariance model with different trends
    for &trend_fn in &trend_functions {
        let kriging = EnhancedKrigingBuilder::new()
            .points(points.clone())
            .values(values.clone())
            .covariance_function(CovarianceFunction::Matern52)
            .trend_function(trend_fn)
            .optimize_parameters(true)
            .build()?;

        let cv_error = cross_validate(&kriging, points, values, 5)?;

        println!("Matern52 + {:?}\t{:.4}", trend_fn, cv_error);
    }

    Ok(())
}

/// Helper function to print distribution quantiles
fn print_quantiles(samples: &ArrayView1<f64>) {
    let mut sorted_samples = samples.to_owned();
    sorted_samples
        .as_slice_mut()
        .unwrap()
        .sort_by(|a, b| a.partial_cmp(b).unwrap());

    let n = sorted_samples.len();
    let quantiles = [0.025, 0.25, 0.5, 0.75, 0.975];

    for &q in &quantiles {
        let idx = (q * (n as f64)) as usize;
        let idx = idx.min(n - 1);
        println!("  {:.1}%: {:.4}", q * 100.0, sorted_samples[idx]);
    }
}

/// Extension trait to calculate mean and standard deviation for Array1
trait ArrayStats {
    fn mean(&self) -> f64;
    fn std(&self, ddof: f64) -> f64;
}

impl ArrayStats for ArrayView1<'_, f64> {
    fn mean(&self) -> f64 {
        let sum: f64 = self.iter().sum();
        sum / (self.len() as f64)
    }

    fn std(&self, ddof: f64) -> f64 {
        let mean = self.mean();
        let variance =
            self.iter().map(|&x| (x - mean).powi(2)).sum::<f64>() / (self.len() as f64 - ddof);
        variance.sqrt()
    }
}

/// Perform k-fold cross-validation on a kriging model
fn cross_validate<F>(
    model: &EnhancedKriging<F>,
    points: &Array2<F>,
    values: &Array1<F>,
    k_folds: usize,
) -> Result<F, Box<dyn std::error::Error>>
where
    F: Float + FromPrimitive + Debug,
{
    let n = points.shape()[0];
    let fold_size = n / k_folds;
    let mut errors = Vec::with_capacity(n);

    for i in 0..k_folds {
        // Create training/test split
        let test_start = i * fold_size;
        let test_end = if i == k_folds - 1 {
            n
        } else {
            (i + 1) * fold_size
        };

        let mut train_points = Vec::with_capacity(n - (test_end - test_start));
        let mut train_values = Vec::with_capacity(n - (test_end - test_start));
        let mut test_points = Vec::with_capacity(test_end - test_start);
        let mut test_values = Vec::with_capacity(test_end - test_start);

        for j in 0..n {
            if j >= test_start && j < test_end {
                // Test set
                test_points.push(points.slice(s![j, ..]).to_owned());
                test_values.push(values[j]);
            } else {
                // Training set
                train_points.push(points.slice(s![j, ..]).to_owned());
                train_values.push(values[j]);
            }
        }

        // Convert to ndarray
        let train_points = Array2::from_shape_vec(
            (train_points.len(), points.shape()[1]),
            train_points.into_iter().flatten().collect(),
        )?;

        let train_values = Array1::from_vec(train_values);

        let test_points = Array2::from_shape_vec(
            (test_points.len(), points.shape()[1]),
            test_points.into_iter().flatten().collect(),
        )?;

        let test_values = Array1::from_vec(test_values);

        // Fit model on training data
        let cv_model = model.refit(&train_points, &train_values)?;

        // Predict test data
        let preds = cv_model.predict(&test_points)?;

        // Calculate errors
        for j in 0..test_values.len() {
            let error = preds.value[j] - test_values[j];
            errors.push(error * error);
        }
    }

    // Calculate mean squared error
    let mse = errors.iter().sum::<F>() / F::from_usize(errors.len()).unwrap();

    Ok(mse)
}
